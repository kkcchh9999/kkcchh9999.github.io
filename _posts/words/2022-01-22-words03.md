---
title:  "Continual Learning on the Edge with TensorFlow Lite 용어"
excerpt: "논문에서 나오는 용어 정리"
categories:
  - Words
tags:
  - [words]
toc: true
toc_sticky: true
sidebar: 
  nav: "docs"
---

### 용어 정리
논문을 읽다가 어려웠던 용어, 약어들을 정리헀다.

---
- `transfer learning`: 전이학습, 기존의 신경망에서 약간의 수정과 소수의 데이터을 통해 새로운 클래스를 학습하는 기법
- `catastrophic forgetting`: 파괴적 망각, 새로운 데이터로만 학습할 경우 모델이 이전에 학습한 내용을 망각하는 현상  
- `CORe50 Benchmark`: 연속학습을 테스트하기 위한 벤치마크 
- `Batch Normalization`: 배치 정규화, batch normalization은 학습 과정에서 각 배치 단위 별로 데이터가 다양한 분포를 가지더라도 각 배치별로 평균과 분산을 이용해 정규화하는 것
- `Fully Connected Layer`: 완전히 연결 되었다라는 뜻으로, 한층의 모든 뉴런이 다음층이 모든 뉴런과 연결된 상태, 2차원의 배열 형태 이미지를 1차원의 평탄화 작업을 통해 이미지를 분류하는데 사용되는 계층이다. 2차원 배열 형태의 이미지를 1차원 배열로 평탄화하고, 활성화 함수(Relu, Leaky Relu, Tanh,등)로 뉴런을 활성화하고, 분류기(Softmax) 함수로 분류하는 과정
- `Softmax function`: 소프트맥스 함수는 로지스틱 함수의 다차원 일반화이다. 다항 로지스틱 회귀에서 쓰이고, 인공신경망에서 확률분포를 얻기 위한 마지막 활성함수로 많이 사용된다. 이름과 달리 최댓값(max) 함수를 매끄럽거나 부드럽게 한 것이 아니라, 최댓값의 인수인 원핫 형태의 arg max 함수를 매끄럽게 한 것이다. 그 계산 방법은 입력값을 자연로그의 밑을 밑으로 한 지수 함수를 취한 뒤 그 지수함수의 합으로 나눠주는 것이다.
- `Replay Buffer`: 재현 버퍼, 학습중에 이전의 경험을 저장했다가 추후에 다시 실행할 수 있도록 하는 버퍼.