---
title:  "AI Benchmark: Running Deep Neural Networks on Android Smartphones 번역"
excerpt:  "AI 벤치마크: 안드로이드 스마트폰에서 심층 신경망 구동하기 논문 번역"
categories:
  - Papers
tags:
  - [android, papers]
toc: true
toc_sticky: true
sidebar: 
  nav: "docs"
---

## 논문 번역
NosLab 인턴 활동 기간동안 읽은 논문을 번역했다. 저작권을 염려하여 그림은 삽입하지 않았다.   
원 논문은 [https://arxiv.org/abs/1810.01109](https://arxiv.org/abs/1810.01109) 에서 찾아볼 수 있다.  
저자는 Andrey Ignatov, Radu Timofte, William Chou, Ke Wang, Max Wu, Tim Hartley, Luc Van Gool 이다. 
# AI 벤치마크: 안드로이드 스마트폰에서 심층 신경망 구동하기
## AI Benchmark: Running Deep Neural Networks on Android Smartphones

## 요약
지난 몇년동안 스마트폰과 태블릿같은 모바일 기기들의 계산 능력이 비약적으로 증가하여 얼마 지나지 않은 데스크탑의 수준에 도달했다.
일반적인 스마트폰의 어플리케이션이 그들에게 더이상 문제가 되지 않지만, 여전히 최고 성능의 기기도 쉽게 도전하기 어려운 분야가 남아있는데, 즉 인공지능 알고리즘이다.
이 논문에서는, 우리는 안드로이드 생태계에서 딥러닝의 현 상태에 대한 연구를 시사하고, 이용가능한 프레임워크와 프로그래밍 모델, 스마트폰에서의 AI 에 대한 한계를 설명한다.
우리는 네가지 주요 모바일 칩셋 플랫폼에서 사용할 수 있는 하드웨어 가속 리소스에 대한 개요를 제공한다: Qualcomm, HiSilicon, MediaTek, Samsung. 
추가적으로, 기존의 모든 주요 하드웨어 구성에 적용되는 AI Benchimark 로 수집된 다양한 모바일 SoC의 실제 성능 결과를 제시한다.  

## 1 도입
휴대전화의 SoC 기술의 발전과 함께, 이동 가능한 안드로이드 기기의 퍼포먼스가 지난 몇년간 배로 상승했다. 
멀티 코어 프로세서, 전용 GPU, 기가바이트 RAM을 갖춘 현재 스마트폰의 기능은 이미 애플리케이션이나 간단한 모바일 게임에 내장된 표준을 훨씬 뛰어넘었다.
그들의 연산 능력이 대부분의 요구사항에 비해 상당히 초과했지만, 여전히 인공지능 알고리즘은 고급의 스마트폰과 태블릿에게 마저 어려운 과제로 남아있다.
많은 머신러닝 프로그램들이 사용자들에게 배포될 때 매우 유용하다는 사실에도 불구하고, 모바일 플랫폼에서 그들을 가동하는것은 모바일 CPU의 거대한 계산 오버헤드와 상당한 배터리 소모와 연관되어있다.  
 
그러나 최근의 딥러닝 발전은 모바일 기기를 위한 작업과 밀접되어 있다. 
주목할 만한 이런 기능의 한 그룹은 이미지 분류, 이미지 향상, 동영상 향상, 광학 문자 인식, 물체 추적, 장면 이해, 얼굴 탐지 및 인식, 시선 추적 등등과 같은 컴퓨터 비전 문제에 관한 문제이다. 
다른 그룹은 자연어 번역, 문장 완성 검사, 감성 분석, 반응형 챗봇과 같은 다양한 자연어 처리에 관한 문제를 포함한다.
다른 기능의 그룹은 제스처 인식, 수면 모니터링 등과 같은 가속도계 데이터로부터 인간의 신체를 인식하기 위한 온라인 센서 데이터 처리를 다룬다.
몇몇 다른 딥러닝 문제 예를 들어 음성 인식이나 가상 현실 등 또한 스마트폰과 연관되어 있다.   

모바일 어플리케이션을 위한 딥러닝의 관심이 증가함에도 불구하고, 앞서 말한 하드웨어적 제약으로 인해 많은 주요 AI 알고리즘이 스마트폰에서 사용 불가능하거나, 원격 서버에서 실행된다.
다음 제약으로부터 또한 자유롭지 못하다: a) 개인정보 문제 b) 인터넷 연결 의존성 c) 네트워크 지연성에 의한 딜레이 d) 병목 현상 - 사용 가능한 클라이언트 수는 서버의 연산 능력에 달려있다.
이런 문제들을 극복하기 위해, GPU나 DSP를 사용한 하드웨어 가속을 통해 알고리즘의 일부 혹은 전체 머신 러닝 라이브러리를 모바일 플랫폼으로 이식하려는 많은 시도가 있었다.
[29]에서 저자들은 Qualcomm의 Hexagon DSP에서 센서 추론 작업이 가능한 모바일 신경망 분류 엔진 개발했다.
그들은 매우 인상적인 에너지 소비 결과를 달성했지만, 작은 프로그램과 메모리 공간으로 인해 DSP는 매우 작은 CNN모델만을 실행할 수 있었다.
[31]에서, 저자들은 모바일 GPU에서 사전 훈련된 CNN의 병렬 실행을 위한 GPU 가속 라이브러리 CNNdroid를 제시했다.
이는 CPU와 GPU 사이로 연산을 병렬로 진행하는 RenderScript framework를 기반으로 하고, 제안된 기법은 기본 싱글 쓰레드 기법보다 40배 빨랐지만, 실제로 속도는 Arm NEON 명령어 세트에 의존하는 CPU 기반 TensorFlow Mobile 라이브러리와 비교되었다. 
그 외 Motamedi는 RenderScript를 사용하는 것과 동일한 접근 방식을 이용했지만 CPU의 부정확한 컴퓨팅 모드를 사용하여 실행 시간을 줄였다.
유망한 결과에도 불구하고, 정확하지 않은 연산이 가지는 효과의 정확성은 이 논문에서 깊이 조사되지 않았으며 따라서 이 접근 방식의 적용 가능성은 여전히 불분명하다.
RSTensorFlow는 GPU기반 행렬 연산 가속을 위해 RenderScript를 사용하는 다른 시도로, 여기서는 TensorFlow Mobile 라이브러리를 직접 수정하여 사용한다.
이는 행렬곱 연산이 최대 3배 빨라질 수 있다는것과 총 추론시간의 75%를 차지하는 합성곱 연산의 속도를 높이는 것은 불가능하다는 것을 증명했다.
또한, 해당 실험은 RenderScript가 항상 GPU를 사용하는 것은 아니고, 때로는 CPU에서만 연산을 수행하기 때문에, 기존의 TensorFlow 구현보다 실행 시간이 길어지기도 한다는 것을 나타낸다.  

그 외에도 컴퓨팅 집약적인 작업을 실행하기 위한 일부 SDK(Software Devlepment Kit)는 SoC 제조업체에서 직접 제안했다.
2016년에, Qualcomm은 그들의 GPU와 DSP를 통한 인공 신경망 실행 가속을 위한 Snapdragon Neural Processing Engine(SNPE)를 소개했다.
다음 해에, HiSilicon은 인공 신경망을 Kirin's NPU에서 실행하기 위한 HiAI 플랫폼을 제안했고, 이후 MediaTek는 딥러닝 모델을 실행하기 위해 GPU 또는 APU를 동작하게 할 수 있는 NeuroPilot SDK를 발표했다.
가장 큰 화제가 된 것은 이 SDK들이 각각에 상응하는 칩셋에서만 기능한다는 것이다.
즉 HiAI에 의존하는 어플리케이션은 Qualcomm SoC에서 동작하지 않고, 반대도 마찬가지이다.
이는 개발자들을 그들의 어플리케이션을 각 플랫폼에 맞는 버전을 생성하거나, 일부를 포기하도록 강제한다.
이 상황은 모바일 기기에서 딥러닝 모델을 실행시킬 수 있도록 디자인된 Android Neural Networks API(NNAPI)가 소개되면서 바뀌었다.
이 API는 기본적으로 상위 레벨의 머신러닝 프레임워크와 장치의 하드웨어 가속 자원의 중간 레이어이며, 가장 적합한 하드웨어에서의 태스크 실행의 스케줄링과 레이어간의 통신을 담당한다.
NNAPI는 여전히 CPU 이외의 다른 장치에서 계산을 실행하기 위해 특정 SoC 공급업체의 드라이버가 필요하기 때문에, Android 8.1 이상의 버전에서는 하드웨어 가속 지원을 자동으로 보장하지 않는다.  

많은 모바일 기기의 CPU와 GPU에 대한 벤치마킹 테스트가 존재하지만, 사용 가능한 AI 칩 및 DSP로 성취할 수 있는 AI 연산의 가속과 속도 측정에 대한 내용은 존재하지 않는다.
이 논문에서는 머신 러닝 퍼포먼스, 사용 가능한 AI 하드웨어 가속, 칩셋 드라이버 그리고 현대 안드로이드 스마트폰의 메모리 한계를 테스트하기 위해 특별히 고안된 AI 벤치마킹을 제시한다.
이는 모바일 기기의 하드웨어에서 직접 실행된 여러 컴퓨터 비전 AI 테스트 그리고 딥러닝 관련 아키텍처 및 명령으로 구성된다.
우리는 실제 칩셋 제조사들과 인기있는 모바일 머신러닝 프레임워크들에 대한 상세한 설명을 제시하고, 스마트폰에서의 딥러닝 알고리즘에 대한 한계를 제시한다.
우리는 10000 종료가 넘는 스마트폰과 태블릿을 위해 우리의 AI 벤치마크를 통해 수집된 약 200개의 안드로이드 기기와 주요 모바일 칩셋의 실제 성능을 보여준다.  

문서의 나머지는 다음 내용과 같다. 
Section 2에서 우리는 메인 칩셋 플랫폼에서 사용 가능한 하드웨어 가속 리소스와 칩셋에 접근하기 위한 프로그래밍 인터페이스에 대해 설명한다.
Section 3에서는 인기있는 모바일 딥러닝 프레임워크에 대한 개요를 보여준다. 
Section 4에서는 벤치마크 아키텍처의 프로그래밍 구현, 포함하는 컴퓨터 비전 테스트를 자세하게 설명한다.
Section 5에서는 다양한 안드로이드 장치와 칩셋에서의 다양한 딥러닝 구조에 따른 실험결과와 추론 시간을 보여준다.
Section 6은 얻은 결과를 분석한다.
마지막으로 Section 7은 논문의 결론이다.  

## 2 하드웨어 가속
최초의 컴퓨터는 대부분 단일 독립 실행형 CPU를 탑재했고, 이는 곧 다양한 멀티미디어 프로그램에 대해서 너무 제한적인 계산 성능을 보인다는것이 명확해졌다. 이는 주 CPU와 함께 병렬적으로 동작하는 보조 프로세서의 제작을 이끌었다. 그들의 구조는 많은 싱글 프로세싱 작업에 최적화 되었다. 1980년대 초에 NEC PD7720, AT&T DSPI 그리고 TI TMS32010 보조 프로세서들이 소개되면서, 디지털 신호 처리기(DSPs)의 시대가 시작되었다. 그들은 지금까지 사용되고 있는 DSP 구조의 일반적인 원칙을 설립했다: Harvard architecture, MAC 연산을 위한 하드웨어 블록, 병렬 연산을 위한 VLIW와 SIMD 명령어 셋 등등. 최초의 DSP들은 그들의 명령어 셋과 메모리 제약으로 인한 상당히 제한된 능력에도 불구하고, 90년대 중반까지 널리 사용되었다. 그들은 컴퓨터 그래픽, 사운드 비디오 디코딩 등과 관련된 어플리케이션, 다양한 사진 편집 소프트웨어의 수학적 보조 프로세서 및 가속기로 널리 사용되며, 1989년에 설계된 최초의 딥 러닝 OCR 모델 실행에도 사용되었다. 이후 벡터 및 행렬 기반 연산으로 인해 CNN을 사용한 손으로 쓰여진 숫자 분류에서 그 당시 가장 빠른 속도(초당 12 이미지)에 도달했다. 이는 고도로 병렬화 가능한 DSP 구조와 MAC operation의 하드웨어 구현을 초래했다. 90년도 후반에는 DSP의 인기가 감소하기 시작하고, 소비자 PC부문에서는 대부분 DSP 명령어가 통합된 범용 CPU, 효율적인 병렬 계산을 위한 GPU 및 다양한 특정 문제에 대해 구성 가능한 FPGA로 대체되었다.   

1990년대 초에, DSP는 전화기에서 나타나기 시작했다. 그들은 최초로 음성 코딩, 압축 뿐만 아니라 일부 무선 신호 처리에  사용되었다. 나중에, 모바일 장치의 카메라와 음악, 비디오 재생같은 많은 기능들의 구현으로, 이미지, 비디오, 음성 처리를 위한 통합 DSP가 널리 사용되기 시작했다. 데스크톱 컴퓨터에서 일어난 일과 대조적으로, DSP는 CPU와 GPU로 변경되지 않았는데 이는 모바일 기기에서 중요한 낮은 전력 소모분야에서 높은 성능을 보였기 때문이다. 모바일 기기는 최근에는 DSP와 SoC 요소의 계산 능력이 상당히 증가했고, 지금은 GPU와 NPU, 전용 AI 코어로 보완되어 AI와 딥러닝 기반 연산을 할 수 있다. 현대 모바일 기기의 상세한 묘사는 그림 1에 나타나고, 하드웨어 가속 자원은 이후에 나타낸다.

### 2.1 Qualcomm 칩셋/ SNPE SDK
Qualcomm(이하 퀄컴)은 1985년에 설립된 미국의 반도체와 무선통신 회사이다. 최초의 SnapDragon 모바일 SoC QSD8250은 2007년에 배포되었고, 이미 전용 AMD Z430 GPU와 첫 번째 상업용 QDSP6 Hexagon DsP를 갖추고 있었다. 2009년에는, AMD의 모바일 그래픽 부서를 인수한 뒤, 동일 GPU 시리즈의 이름이 Adreno(anagram from Radeon)으로 변경되었고, 현재 모든 Snapdragon SoC에는 이 이름으로 후속 제품이 있다. 그들의 성능은 2.1(Adreno 200)에서 727(Adreno 630) GFLOPS 까지 상승했다. DSP 구조 또한 최초(2006)에서 현재 6세대까지 많은 변화를 겪었다. 이제는 wide vector extensions(HVX), 동적 멀티쓰레딩, VLIW, SIMD 명령어 셋을 지원한다. 이들은 유저로부터 프로그램 될 수 있다. 주 Snapdragon CPU 코어는 Arm 아키텍처 기반이고, 보통 퀄컴의 Arm Cortex 코어를 기반으로 개발된 Qualcomm의 자체 맞춤형 내부 설계를 특징으로 한다. Snapdragon의 세 다른 컴퓨터 구조 컴포넌트(Arm NEON 명령어 셋 기반 CPUs, GPUs, DSPs)는 많은 AI 알고리즘을 수행하기에 적합하다(그림 2). 퀄컴 칩셋은 이제 55%의 스마트폰 SoC 시장을 차지하고 있고, 많은 인기있는 스마트폰, 태블릿과 wearable 기기에 적용되어 있다.   

퀄컴은 모바일 장치에서의 AI 추론 하드웨어 가속에 대해 2015년 5월에 Snapdragon 820에서 처음으로 다루었고, 2016년 5월, Snapdragon의 모든 컴포넌트에서 런타임 가속을 지원하는 SNPE(Snapdragon Neural Processing Engine)를 공표했다. 이 SDK는 일반적인 Caffe/Caffe2, TensorFlow, PyTorch, Chainer, MxNet, CNTK 그리고 ONNX를 통한 PaddlePaddle 같은 딥러닝 모델 프레임워크를 지원한다. 이는 개발자들이 그들이 제작한 딥러닝 모델을 다양한 퀄컴 기기에서 실행할 수 있게 설계되었다. 이 SDK는 17개의 Snapdragon 모바일 프로세서에서 지원된다: 프리미엄 모델 845, 835, 820, 상위 모델 710, 670, 660, 652, 650, 653, 636, 632, 630, 626, 625, 중급 모델 450, 539, 429. 이는 또한 퀄컴 비전 정보 플랫폼 QCS603, QCS605에서도 지원하고, IoT 장치에서의 효율적인 머신러닝을 위해 고안되었다.  

Hexagon DSP 에서의 딥러닝의 양자화를 위한 퀄컴의 최초 NNAPI 드라이버는 Android O-MR1에서 소개되었다. 이는 당시에는 어떤 상품에서도 사용되지 않았지만, OnePlus 6, Xiaomi Mi8에서 다음 안드로이드 버전과 함께 처음 나타났다. Android P에서, 이 드라이버는 float 모델들을 Adreno GPU에서 실행할 수 있는 기능을 추가적으로 지원했다. 하지만 현재 이는 시장에서 찾을 수 없다. NNAPI 드라이버는 일반적으로 하드웨어 가속 원칙을 채택하고, SNPE SDK 구현에 사용된다.차이는 주로 현재 Android NNAPI 설계의 제한으로부터 나타난다. 퀄컴은 OEM 고객에게 제공된 소프트웨어 이미지로 이런 드라이버를 제공하고, 최초 릴리스 시기와 장치에 포함시킬 방법을 결정한다: 최초 릴리즈, 이후 소프트웨어 업데이트. 결과적으로, 그들의 존재와 실제 버전은 크게 다를 수 있다.   

### 2.2 HiSilicon 칩셋/ Huawei HiAI SDK
HiSilicon(이하 하이실리콘)은 중국의 화웨이가 2004년에 설립한 반도체 자회사이다. 그들의 최초 모바일 프로세서(K3V1)은 2008년에 소개되었지만 최초의 상업적인 성공은 2012년에 배포했고, 네가지 Arm CortexA-9 CPU 코어와 Vivante GPU를 특징으로 하는 다음 세대의 SoC(K3V2)이다. 2014년에는 K3 시리즈의 후속으로 중급 모델(600시리즈)과 하이엔드 모델(900시리즈)칩셋으로 구성된 새로운 Kirin SoC 제품군이 출시되었고, 지금까지 화웨이 장치에서 사용 중이다. 퀄컴과 달리 하이실리콘은 맞춤형 CPU와 GPU를 제작하지 않고, 모든 Kirin 칩셋은 Arm Cortex CPU 코어와 다양한 버전의 말리 GPU를 기반으로 한다. AI 계산을 위한 다른 접근법 또한 개발되었다: GPU와 DSP에 의존하는 대신, 하이실리콘은 AI와 딥러닝 알고리즘 연산에 널리 사용되는 빠른 벡터와 행렬 기반 연산에 특화된 NPU(neural processing unit)을 도입했다. 화웨이에 따르면 이는 일반적인 쿼드코어 Cortex-A73 CPU 클러스터와 비교했을 때 최대 25배 향상된 성능을 제공하고, 50배의 효율성을 보인다. NPU 디자인은 라이센스가 Cambricon Technologies company (Cambricon-1A chip)에 있고, 이는 양자회된 8비트 계산을 할 때, 최고 1.92 TFLOPS의 성능을 보인다. 이 NPU는 Kirin 970 SoC에서 처음 나타났다. 이후 다른 강화된 두 NPU들은 후속 Kirin 980 칩셋에 통합되었다. Kirin 970/980과 별도로 다른 SoC들은 이 NPU 모듈을 포함하지 않고, 현재 써드파티 AI 기반 어플리케이션을 위한 하드웨어 가속을 지원하지 않는다는 것을 주목해야 한다. 앞서 언급한 침셋은 오직 화웨이 장치 내부에서만 찾아볼 수 있는데, 이는 그들이 외부 OEM 업체에 판매하지 않기 때문이다: 현재 총 시장의 하이실리콘 SOC의 비율은 약 10% 정도이다.   

Kirin의 NPU에 외부 접근을 허용하기 위해, 화웨이는 2017년 후반에 HiAI Mobile Computing Platform SDK를 배포했다. 이는 Kirin SoC에 통합된 하드웨어 자원 위에서 딥러닝 모델을 실행하기 위한 API를 제공한다. 이 SDK는 현재 Caffe와 Tensorflow Mobile, Lite frameworks만 지원하는데, 향후에는 Caffe2와 ONNX를 지원할 수도 있다. 이는 16비트, 8비트, 1비트로 양자화된 모델의 가속을 제공하고, 추가적으로 0 변수를 포함하는 multiply-add를 건너뛰는 방법을 통해 sparse model의 속도를 향상시킬 수 있다. 낮은 레벨의 API와 별개로, HiAI 엔진은 이미지 분류, 얼굴 및 얼굴 속성 감지, 문서 탐지 및 수정, 초해상화, QR코드 탐지 등의 바로 사용 가능한 컴퓨터 비전 알고리즘을 제공한다.  

안드로이드 8.1을 시작으로, 화웨이는 HiAI를 기반으로 하는 NNAPI 드라이버가 Kirin 970/980에 포함되었다. 현재, 그들은 오직 16비트 모델만을 지원하고, 양자화 네트워크는 미래의 릴리즈 버전에 지원될 것이다. 다른 칩셋을 사용하는 화웨이 장치들은 NNAPI 드라이버를 지원하지 않는다. 이는 이전에 설명한대로 NPU 모듈이 사용되지 않기 때문이다. 

### 2.3 MediaTek 칩셋/ NeuroPilot SDK 
MediaTek(이하 미디어테크) United Microelectronics Corporation에서 1997년에 분사한 대만의 반도체 회사이다. 이의 모바일 사업부는 2004년에 창설되었고, 곧 미디어테크는 그들의 모바일 칩셋을 배포했다. 이는 당시 생산된 많은 보급형 중국 전화기들에 사용되었다. 미디어테크는 2013년에 각각 듀얼, 쿼드코어 SoC 657x, 658x 그리고 Mali, PowerVR 그래픽을 소개함으로 글로벌 시장에서 인기를 얻었다. 그리고 64비트 MT67xx 칩셋의 배포로 다양한 OEM을 통해 안드로이드 장치들에서 널리 사용되게 되었고, 시장 점유율의 20%를 차지했다. 화웨이와 비슷하게, 미디어테크는 SoC를 표준 Arm Cortex CPU 코어와 Mali 혹은 PowerVR GPU에 통합되고 있다. 2018년 초에 미디어테크는 머신러닝 기반 어플리케이션의 가속 문제를 임베디드 AI 처리 유닛(APU)과 함께 Helio P60플랫폼을 출시하며 해결했다. 이 APU는 8비트 연산에서 280GMAC까지 도달할 수 있고, 주로 양자화된 딥러닝 가속에 사용된다. 이는 네개의 Cortex-A53 CPU 코어와 800MHz 클록의 Mali-G72 MP3 GPU에서 구동된다. 따라서 미디어테크의 접근은 화웨이와 퀄컴의 중간에 놓여있다: 양자화 연산을 위한 전용 칩(Kirin 칩셋의 방법) 그리고 float를 위한 CPU/GPU 사용(Snapdragon 칩셋에서의 방법).   

Helio P60의 배포는 미디어테크의 Tensorflow Lite(이하 TF Lite)와 NNAPI를 중심으로 구현된 NeuroPilot SDK와 동반되었다. 이 SDK는 4개의 주 컴포넌트로 이루어 져 있다: 1) TF Lite 네트워크와 학습된 TensorFlow/Caffe/ONNX 모델의 양자화를 위한 TOCO 기반 툴. 2) 구현된 TF Lite 작업과 변환된 .tfite 모델을 로드, 실행하기 위한 해당 인터프리터의 확장 목록. 3) 미디어테크의 NeuroPilot를 위한 하드웨어 가속 명령을 구현하는 APU, GPU NNAPI 드라이버: 현재, APU 드라이버는 오직 INT8을 지원하고, GPU 드라이버는 FP16/32 옵션을 지원한다. 4) 딥러닝 기반 어플리케이션을 디버깅하고 프로파일링 하기 위한 부분과 GPU 또는 APU와 같은 특정 하드웨어 가속기에서 대상 작업을 고정하기 위한 인터페이스. 해당 SDK는 미디어테크 NeuroPilot 호환 칩세에서만 지원된다.(현재는 Helio P60이 유일하다)   

양자화 모델, 연산을 지원하는 독립형 버전의 NNAPI 드라이버 또한 존재한다. 그럼에도 불구하고, P60 개발자 플랫폼을 제외하고, 오직 하나의 상용화된 장치가 존재한다. TekP60 칩셋에는 이 드라이버가 포함된 것으로 알려진다.   

### 2.4 Sansung chipsets
삼성전자는 1969년에 설립된 한국의 전자 회사이다. 1988년에, 삼성 반도체와 통신을 합병했고, 현재의 이름을 유지했다. 같은 해, 그들의 첫 모바일 프로세서(S3C44B0, 66MHz, Armv4)는 2000년도에 보여졌음에도 불구하고, 그들의 첫 휴대전화를 발행했다. 이후 많은 Window 모바일 장치와 iPhone 2G/3/3GS, 몇몇의 이른 안드로이드 스마트폰에 사용된 S3Cxxxx와 S5Pxxxx SoC 시리즈를 상당히 확장했다. 2010년 S5PC110 칩셋의 소개와 함께, 모든 삼성 SoC들은 Exynos로 다시 브랜딩 했고, 이 이름을 현재까지 사용하고 있다(Exynos 3 - 9th generations). 화웨이와 미디어테크와 비슷하게, 삼성은 주로 Arm Cortex CPU 코어와 Mail 또는 PowerVR 그래픽을 칩셋에 사용한다. 자체 개발한 Arm 기반 CPU 코어 Mongoose코어를 하이엔드 SoC에 Exynos 8부터 통합해서 사용하고있다. 삼성의 Exynos 8895는 Vision Processing Unit(VPU)로, 주로 카메라를 위해 사용되는 AI 칩으로 소개되었다. 그러나, 드라이버, SDK, 또는 추가 세부정보가 공개되지 않아서 써드파티 어플리케이션에서 접근할 수 없다. 오직 2개의 삼성 디바이스(Note 9, Tab S4)에서만 현재 Android 8.1 이상을 사용 중이고, CPU만 사용하는 구글의 기본 NNAPI 드라이버를 사용한다. 몇몇 루머에 따르면, 다음 세대 Exynos 칩셋은 전용 AI 칩을 포함할 수도 있다. 그러나 해당 정보는 삼성에서 공식적으로 발표한 내용은 아니다. 현재 삼성 칩셋의 시장 점유율은 10% 정도이다.

### 2.5 Google Pixel/ Pixel Visual Core
이들의 안드로이드 OS와 별개로, 구글은 안드로이드 2.1 부터 매년 스마트폰과 태블릿을 구글 Nexus 브랜드 아래에서 출시하기 시작했다. 이는 매번 많은 다른 외부 OEM과 함께 협력하고 개발되었다: 삼성, LG, Motorola, 화웨이, HTC, Asus. 이 장치들은 하이엔드 하드웨어에서 실행되는 스톡 안드로이드 OS를 특징으로 하고, 안드로이드 업데이트를 가장 먼저 받는다(베타 버전을 설치하는 것 또한 가능하다). 2016년에, 모든 Nexus 제품들은 생산 중단되었고, 새로운 스마트폰들은 이전에 말한 원칙들을 유지한 채로 Google Pixel 브랜드 아래에 출시되기 시작했다. 대부분의 이 기기들은 퀄컴 칩셋을 사용하고, 따라서 퀄컴 단락에서 설명한 정보는 이들에게 똑같이 적용된다. Pixel 2(XL)부터 시작하여, 구글은 그들의 스마트폰에 주 퀄컴 SoC와 분리하고, Intel과 협력한 완전한 프로그래밍 가능한 Pixel Visual Core AI chip을 추가했다(그림 4). 이 칩셋은 메인 어플리케이션 프로세서와 통신을 조절하기 위한 Arm Cortex-A53 코어를 하나 포함하고, 통합된 LPDDR4 RAM과 8개의 커스텀 IPU 코어를 포함한다. 각 IPU는 512개의 수리 논리 유닛과 256개의 처리장치가 16X16 2차원 배열로 배열되어있고, 커스텀 VLIW 명령어 셋을 지원한다. 해당 칩은 8비트와 16비트 정수 연산을 지원하고, 3.2 TFLOPS까지의 성능을 보인다.픽셀 비주얼 코어는 일반적으로 TF(Lite)를 호환하지만, 구글은 해당 SDK와 NNAPI 드라이버를 출시하지 않아 외부 개발자가 머신러닝 기반 애플리케이션 가속에 사용할 수 없으며 현재 주로 구글의 HDR+ 이미지 처리에 한정되어 있다.
### 2.6 Arm Cortex CPUs/ Mail GPUs/ NN SDK
현재 모든 SoC에 통합된 CPU 코어는 Arm 구조를 기반으로 하고 있고, 머신러닝 어플리케이션을 위한 하드웨어 가속을 지원하지 않는 장치에서는 이 CPU가 모든 AI 알고리즘을 실행한다. 이런 상황에서 연산을 빠르게 하기 위해, Arm은 벡터와 행렬 기반의 연산을 겨냥한 특정한 명령어 셋을 소개했다. 여기에서 가장 주목할 만한 기술은, Arm NEON 이다: Armv7 프로세서에서 처음 도입된 확장 SIMD(Single Instruction Multiple Data)구조. NEON은 기본적으로 현재의 계산을 위한 DSP와 비슷한 명령어로 구성되고, 16x8 bit, 8x16 bit, 4x32 bit, 2x64 bit 정수 연산과 8x16 bit, 4x32 bit, 2x64 bit 실수 연산의 동시 실행을 허용한다. 게다가, Arm은 최근 그들의 단일 Arm CPU 내의 모든 코어를 병렬 계산에 효율적으로 사용할 수 있는 DynamIQ 기술을 발표했고, Armv8.4 마이크로 아키텍처에서 내적 계산을 위한 명령어를 발표했다. 많은 이런 최적화된 명령어들은 구글의 기본 NNAPI 드라이버에 통합되었고, 다른 가속수단이 없다면 CPU를 사용한다.   

이와 별개로 Arm은 모바일 SoC에서의 머신러닝 연산을 가속화 하기 위한 Arm NNSDK를 발표했다. 이는 TensorFlow, Caffe, ONNX, 그리고 TF Lite 파서들과 함께 머신러닝 워크로드에 CPU와 GPU 통로를 모두 제공한다. CPU 측면에서는, 특정 아키텍처에 대한 주요 로우레벨 최적화와 함께, Armv7 이상의 CPU에서 모든 플랫폼과 호환된다. GPU 통로는 Arm Mali GPU들과 Midgard 시리즈(Mali-T6xx 이후 GPGPU가 도입됨) 또는 더 이후의 Bifrost 시리즈(G71/ G51 그리고 이후 시리즈) 모두에서 사용가능하며, 이는 Mali GPU와 OpenCL 드라이버가 설치되어 있어야 한다. Arm NN SDK는 FP32와 양자화된 INT8 네트워크를 모두 지원하고, 리눅스나 안드로이드 플랫폼에서 NNAPI와 병렬적으로 실행할 수 있다.   

### 2.7 Android NNAPI
다른 모바일 플렛폼에서 DSP, GPU, NPU에 접근하기 위한 다양한 소유권이 있는 SDK가 존재한다. 이는 딥러닝 알고리즘을 모바일에서 사용하기 위한 하드웨어 가속의 난제였는데, 모든 SDK들은 일부 칩셋에서만 사용했고, 심지어 몇몇은 서로 양립할 수 없었기 때문이다. 이를 해결하기 위해 구글은 최근에 소개된 통합된 Android Neural Networks API(NNAPI)를 출시했고, 이는 모바일 기기에서 머신러닝과 딥러닝 연산을 집중적으로 실행하기 위한 안드로이드 C API 이다. NNAPI의 시스템 구조는 그림 5에 있다. 어플리케이션들은 보통 NNAPI를 직접 사용하지 않고, 대신 상위의 머신러닝 프레임워크들을 의존하고 이 프레임워크들은 NNAPI를 사용하여 지원하는 장치에서 하드웨어 가속 `추론`을 사용할 수 있다. NNAPI를 사용하는 연산을 수행하기 위해, 실행 모델은 먼저 수행할 계산을 정의하는 방향 그래프로 표현해야 한다. 이 그래프는 데이터 정의 모델과 결합되는데, (예를들어 머신러닝 프레임워크에서 전달된 가중치와 편향)이는 NNAPI 런타임 평가를 위한 모델을 형성한다. 어플리케이션과 장치 하드웨어의 요구사항에 기반하여, 안드로이드 딥러닝은 사용 가능한 장치 내의 딥러닝 전용 칩과 GPU, DSP들을 포함한 프로세서에 계산 작업량을 효율적으로 분산시킬 수 있다. NNAPI는 안드로이드 8.1 이상의 모든 기기에서 사용 가능하지만, 여전히 장치의 하드웨어에 접근하기 위해서 칩셋 제조사의 드라이버가 필요하다. 이 드라이버의 부족으로 인해, NNAPI는 최적화된 코드를 통해 CPU에서 실행한다.   

## 3 딥러닝 모바일 프레임워크
안드로이드 운영체제의 넓은 사용과 함께, 많은 인기있는 딥러닝 프레임워크가 아래 플랫폼들로 이식되었다: Torch, Deeplearning4j, TensorFlow(Mobile, Lite), Caffe, Caffe2, MXNet, NNable 등. 현재, 가장 널리 사용되는 셋은 TensorFlow Mobile, TensorFlow Lite 그리고 Caffe2로 아래에서 설명한다.   

### 3.1 TensorFlow Mobile 
Tensorflow는 2015년에 구글에서 연구및 개발 목적으로 배포한 오픈소스 머신러닝 라이브러리이다. TensorFlow의 프로그래밍 모델은 입력과 출력 변수의 관계를 정의하는 직접 그래프로 설명할 수 있다. 그 그래프는 딥러닝 모델과 상응하는 데이터 흐름 연산을 정의하는 입력 데이터에 순차적으로 적용되는 다영한 연산자를 대표하는 노드의 세트로 구성된다. 모델이 훈련되고 나면, .pb 그래프(모델의 변수 + 구조로 이루어진 바이너리 파일)를 내보낼 수 있고, TensorFlow Mobile(이하 TF Mobile)의 라이브러리를 통해 모바일 기기에서 실행될 수 있다. Java 추론 인터페이스에 사응하는 코드 스니펫은 그림6에 나타난다. 실제 어플리케이션 코드에서 모델 구조를 특정화 할 필요 없다는 사실에 주목하자: 훈련된 가중치가 .pb 그래프와 함께 이미 저장되어 있고, 개발자는 오직 파일의 위치와 입력 데이터만 제공하면 된다.  

TF Mobile의 가장 큰 장점은 기존의 TensorFlow에서 사용 가능한 대부분의 명령을 지원한다는 것이고, 따라서 거의 모든 TensorFlow 모델은 모바일 장치에서 실행될 수 있다. 또한, 현재 SoC 제조사의 SDK(SNPE, HiAI, NeuroPilot)는 이 라이브러리를 위한 (부분적인) 하드웨어 가속을 지원한다. 구글이 TensorFlow Lite 라이브러리의 지지와 함께, 점진적인 TF Mobile의 지원 중단을 공표했기 때문에 이를 이용한 개발은 곧 끝난다. 특히 TF Mobile은 Android NNAPI 지원을 받지 못하기 때문에, 특정한 SDK를 사용하지 않으면 CPU에서만 동작한다.   

### 3.2 TensorFlow Lite 
TensorFlow Lite(이하 TF Lite)는 2017년 말에 TF Mobile의 후계자로  공표되었다. 구글에 따르면, 이는 커널 최적화와 사전에 융합된 활동, 적은 의존성을 통해 더 나은 퍼포먼스와 더 작은 사이즈의 바이너리를 제공한다. TF Mobile과 비슷하게, 미리 훈련된 TensorFlow 모델이 .tflite 포멧으로 변경될 수 있고, 이후 안드로이드와 iOS 플랫폼에서 추론에 사용될 수 있다. 상응하는 자바 코드 스니펫은 그림 6에 나타난다. 파일 형식의 변화(.pd 대신 .tflite)는 파싱/언패킹 절차 없이 저장된 파일에 접근하고, 종종 개체별 메모리 할당과 결합되는 새로운 FlatBuffers 직렬화 라이브러리의 사용을 이끌어 냈다. 마침내 새로운 라이브러리가 Android NNAPI와 호환되며, 적절한 칩셋과 드라이버와 함께 하드웨어 가속을 기본적으로 실행할 수 있다.  

하지만, TF Lite는 현재 개발자 프리뷰 중이며, 여러 실질적인 제약이 있다는 사실을 주의해야 할 필요가 있다. 먼저, 이미지 크기 조정, 배치 및 인스턴스 정규화, LSTM 장치, 일부 통계 함수 또는 지수화 또는 argmax와 같은 간단한 수학적 연산 등의 완전한 지원이 없는 제한된 연산자 집합만 지원한다. 공식적으로, 구글은 오직 세개의 모델을 보장한다: Inception-V3, MobileNet, Smart Reply SSL 알고리즘. 약간의 수정을 통해 다른 많은 딥러닝 모델을 사용할 수 있다. 두번째 문제는 추론 시간과 RAM의 소모량이다. ByteBuffer은 출력값으로 지원하지 않기 때문에, 이미지 대 이미지 변환은 2배 이상 높은 값을 사용한다. 마지막으로, 안정성은 별다른 문제이다. 개발자 버전에서는 몇몇 이슈가 해결되었지만, 현재 공식 버전은 여러 모바일 장치와 모델에서 완벽하게 작동하지 않을 수도 있다. 아마도 새로운 버전을 통해 많은 문제들이 극복될 수 있을 것이지만, 현재는 이 문제점들이 TF Lite를 사용하기 복잡하게 한다.  

### 3.3 Caffe2 
Caffe는 UC Berkeley에서 Yangqing Jia가 2013년에 배포한 또다른 오픈소스 딥러닝 프레임워크이다. 최초의 비공식적 안드로이드 포트는 이듬해에 나타났다. 그리고 2017년에, Facebook이 후속작 Caffe2와 iOS와 안드로이드를 위한 모바일 버전을 함께 출시했다. Caffe2는 TensorFlow와 비슷한 프로그래밍 모델을 사용한다. 이는 다양한 연산자들을 나타내는 노드와 정적 계산 그래프를 나타낸다. Caffe2 깃허브 레포지토리에 따르면, 이의 모바일 라이브러리의 속도는 보통 TF Lite에 비교할 만 하다(175 ms vs 158 ms, Sanpdragon 821 SoC에서 SqueezeNet model을 구동할 때). [59]리포트에서는 OpenGL backend를 GPU 기반의 연산에서 사용하면, 속도를 6배 증가시킬 수 있다고 주장한다. 하지만 이 기능은 현재의 Caffe2 버전에서는 아직 사용할 수 없다. TensorFlow와 비슷하게, Caffe2 모델을 위한 가속 또한 모든(SNPE, HiAI, NeuroPilot, ArmNN)SDK에서 지원한다. 하지만 NNAPI 지원은 아직 개발중이며, 아직 완전히 통합되지 않았다.   

## 4. AI Benchmark
AI Benchmark(이하 AI 벤치마크)는 AI와 딥러닝 알고리즘을 모바일 플랫폼에서 동작할 때 관련된 메모리 한계와 퍼포먼스를 체크하기 위한 안드로이드 어플리케이션이다. 이는 안드로이드 장치에서 바로 작업하는 인공 신경망을 통해 수행되는 몇몇 컴퓨터 비전 작업으로 이루어져 있다. 사용되는 신경망은 현재 스마트폰에 사용될 수 있는 인기있고 일반적인 아키텍처를 나타낸다. 상세한 내용과 기술적인 세부사항은 아래에서 설명한다. 

### 4.1 딥러닝 테스트
실제 벤치마크 버전 2.0.0은 아래 9가지 딥러닝 테스트로 이루어져 있다.  

#### Test 1: Image Recognition. 
이 작업은 이미지를 1000개의 범주로 분류하는 것이 목표인 전통적인 ImageNet challenge를 나타낸다. 첫 번째 테스트에서, 모바일과 임베디드 비전 어플리케이션을 위해 자원 효율적으로 디자인된 MobileNet-V1 아키텍처로 수행된다. 신경망은 1X1 convolutional(75%), FC(Fully Connected Layer)(24%) 레이어로 구성되어 있고, 총 569M의 multiply add 연산의 95%가 첫번째에서 발생한다. MobileNet는 ImageNet 데이터셋에서 70.6%의 정확도를 달성했는데, 이는 더 큰 AlexNet, SqueezeNet, Inception-V1 모델을 능가한다. 이는 모바일 사용을 위해 양자화를 통해 더 최적화 될 수 있다. 양자화 - 가중치와 활성화를 FLOAT32에서 INT8로 변환하는 것.이는 정확도를 69.7%까지 낮추지만, 동시에 속도를 2배 이상 올릴 수 있고, 사이즈는 4.3MB까지(4배) 줄어든다. 양자화된 MobilNet-V1은 첫 번째 테스트에 배치된다.  

#### Test 2: Image Recognition.
같은 ImageNet 분류 문제로, 위와 같지만 두번째 테스트는 상당히 크고 더 정확하며, 2015년에 구글이 개발한 Inception-V3 CNN을 사용한다. 이는 주로 1 x 1, 1 X 3 + 3 X 1, 1 X 7 + 7 X 1, 3 X 3 convolutional layers로 이루어진 11개의 인셉션 블록으로 구성되어 있다. MobileNet과 대조적으로, Inception-V3은 5000M 가량의 multiply add 연산을 요구하고, 저장된 CNN의 용량은 약 96MB이다. 그러나 같은 ImageNet 데이터셋에서 정확도는 78%로 매우 높은편이고, 현재의 100MB 이하의 인기 신경망 중 가장 높은 정확도를 얻고 있다.   

#### Test 3: Face Recognition.
이 과정의 목표는 데이터베이스에 저장된 얼굴 사진에서 주어진 얼굴과 비슷한 것을 찾는 것이다. 이를 위해 인공 신경망은 먼저 각 얼굴 이미지의 시각적 특징을 인코딩하고 회전, 스케일링, 이동에 불변인 작은 특성 벡터를 제공하게 훈련된다. 이 테스트에서 우리는 구글이 2017년에 선보인 Inception-Resent-V1 신경망을 사용한다. 이는 VGGFace2 데이터셋에서 triplet loss를 줄이도록 훈련되어 있다. 신경망이 훈련되고 나서, 새로운 얼굴 이미지를 입력하면 그에 대한 특성 벡터를 제공한다. 이 벡터는 데이터베이스에서 비슷한 벡터(그리고 각자의 identity)를 검색하는데 사용한다. 해당 작업의 입력 이미지 사이즈는 512 X 512 pixels 이고, 특성 벡터의 차원은 128이다. Inception-ResNet-V1 아키텍처는 20개의 inception 블록으로 이루어 져 있고, 개념적으로 이전에 설명한 Inception V3 CNN과 유사하다; ImageNet 데이터셋에서 그들의 크기와 정확도 그리고 연산 비용도 매우 비슷하다. 가장 큰 이점은 훈련 속도이다 - 이는 Inception-V3와 같은 정확도를 얻기 위해 더 적은 epoch이 필요하다.   

우리는 지금까지 설명한 처음 세 번의 테스트가 모바일 기기에 적합한 classification 문제를 위한 코어 셋 구조라는 점에 주목한다. MobileNet (혹은 그것의 변경) 보다 빠른 신경망은 대체로 더 낮은 정확성을 나타낸다. Inception-V3 혹은 Inception-RestNet-V1보다 나은 정확성을 가진 모델들은 100-150MB의 사이즈를 가지고, 따라서 파일의 크기 때문에 APK 파일에서 응용이 상당히 복잡해진다. 이 신경망들의 양자화는 부분적으로 문제를 해결할 수 있지만, 현재 이들의 양자화 버전은 아직 공개적으로 사용할 수 없다.  

#### Test 4: Imabe Deblurring. 
이 테스트는 SRCNN network를 사용하여 Gaussian blur를 이미지에서 제거하는 것을 겨냥한다. 이 신경망은 현재 많은 이미지 대 이미지 변환 작업의 기준으로 널리 사용되는 초해상도 문제를 위해 고안된 최초의 CNN중 하나이다. 이 신경망의 아키텍처는 매우 얕다: 9 X 9 그리고 5 X 5 필터로 이루어진 3개의 레이어와 총 69,162 파라미터 및 HD 해상도 이미지를 위한 64B multiply-add 명령. 결과적으로, 저장된 미리 훈련된 신경망의 크기는 오직 278KB이다.

#### Test 5: Image Super-Resolution.
이 초해상화의 목표는 축소한 이미지를 원래 버전으로 복구하는 것이다. 이 테스트에서, 우리는 3의 축소 계수를 고려하고, 복구를 위해 VDSR network를 사용했다. 이는 2015년 SRCNN 이후에 곧바로 발표되었다. 이 신경망은 19개의 3 X 3 필터와 covolutional layer로 구성된 VGG 기반의 구조를 특징으로 한다. 이는 많은 이미지 처리에서 최고의 정량적 결과를 얻는다. VDSR 신경망은 665K의 파라미터를 가지고, HD 이미지를 위해 600B정도의 multiply-add 연산을 필요로 한다; 해당 신경망의 크기는 2.7MB이다.  

#### Test 6: Image Super-Resolution.
이 테스트는 이전의 초해상화 문제와 같지만, 축소 계수가 4이고, 두 개의 인공 신경망으로 이루어진 SRGAN 모델을 사용한다. 처음의 하나는 [66]에서 이전에 제안된 ResNet로, 이는 16개의 residual 블록들로 구성되어 있다;이 신경망이 이미지 복원을 수행한다. 두 번째는 adversarial CNN 신경망이다 - 이는 실제 고해상도 이미지와 ResNet이 복원한 이미지를 구분하도록 훈련되었다. 훈련 과정에서 이 신경망들은 아래의 게임을 실행한다: adversarial CNN이 분류의 정확성을 높이려고 시도하는 동안, ResNet은 반대로 이를 최소화 하려고 한다. 즉, 구분할 수 없는 재구성된 이미지를 제공하기 위해서이다. 실제로, 이는 Euclidean norm 혹은 내용 기반보다 훨씬 나은 결과를 만들어 낸다. 모델이 훈련되고 나면, adversarial CNN은 삭제되고, 추론은 오직 RestNet으로만 수행한다. 신경망은 1.5M의 파라미터를 가지고, 학습된 모델의 크기는 6.2MB다. 

#### Test 7: Image Semantic Segmentation. 
이미지 분류에 반해, 이 작업의 목적은 pixel 레벨의 이미지 이해이다. 이는 각 픽셀이 19 카테고리중 하나로 구분되어야 한다는 것을 의미한다: 자동차, 보행자, 하늘, 길 등. 이는 낮은 성능의 기기에서 빠르고 정확한 분류를 위해 고안된 ICNet CNN을 통해서 이루어진다. 속도 증가는 주로 downsampling과 shrinking feature maps를 통해 이루어졌는데, Cityscapes 데이터셋에서의 결과의 정확도는 70.6% mIoU를 기록했다 ICNet은 6.7M개의 파라미터로 구성되어 있고, 훈련된 모델의 크기는 27MB이다.

#### Test 8: Image Enhancement.
우리는 여기서 색 강화, 노이즈 제거, 선명화, 텍스처 합성 등을 포함한 다양한 종류의 개선을 포함하는 일반적인 이미지 및 사진 향상 문제를 고려한다. 이 과정에서의 문제는 DPED 논문에서 처음 다루었는데, 저자들은 저화질의 스마트폰 사진을 DSLR로 촬영한 사진처럼 바꾸려고 했다. 이 작업에서는 ResNet과 비슷하며 4 residual 블록들을 가지고있는 구조를 채택했고, 이미지 폼질의 다양한 측면에서 특정한 손실을 타겟으로 하는것을 제안했다. 얻은 결과는 수동으로 보정하거나, 기존의 알고리즘을 사용했을 때 보다 우수한 품질을 나타낸다는 것을 증명한다. 접근의 주요한 한계점은 장치별로 훈련이 필요하다는 것이다. 이 신경망은 400K의 파라미터를 가지고, 1.6MB의 훈련된 모델 크기를 가진다.  

#### Test 9: Memory Limitations.
이전의 테스트를 진행하는 동안, 주로 다양한 딥러닝 모델을  평가하고, 이 마지막 테스트의 목적은 딥러닝을 실행하기 위해 할당된  RAM 자원을 체크하는것이다. 이 실테스트를 진행할 때, 우리는 Test 4(deblurring)에서 사용한 것과 같은 SRCNN 모델을 사용한다. memory exception이 발생할 때 까지, 점차적으로 입력 이미지의 용량을 증가시킨다. 이는 장치가 큰 입력을 처리할 만큼의 충분한 RAM을 가지고 있지 않다는 것을 의미한다. SRCNN 모델은 속도가 훨씬 더 빠름에도 불구하고, 다른 모델들과 비슷한 RAM을 소모했기 때문에 여기에 채택되었고, 이는 테스트 완료 시간을 줄인다. 신경망에 의해 소비되는 메모리는 주로 가장 큰 계층의 차원에 의해 결정된다는 점에 유의하는것은 용이하다. 여기서는 SRCNN의 경우에 64 convolutional 필터가 첫 번째 필터에 존재한다.   

이 9개의 테스트는 현재의 딥러닝 코어의 벤치마크를 보여준다(그림 7); 기술적 요소 및 구현 세부사항은 아래에 설명한다. 

### 4.2 Technical Description
현재 배포되는 AI Benchmark (2.0.0)은 모든 백그라운드에서 내장 딥러닝 모델을 실행하기 위해 TF Lite 라이브러리를 사용한다. 이전의 버전은 원래 TF Mobile을 기반으로 만들어 졌지만, NNAPI 지원을 하지 않는것이 하드웨어 가속 자원을 사용하는것에 치명적인 제약을 부과하고, 또한 나중에 사장될 것이기 때문에 TF Lite를 사용한다. 실제 벤치마크 버전은 몇몇 이슈가 이미 해결되어 안정화 된 가장 최근의 개발자 버전 TF Lite로 컴파일 되었다.  

벤치마크는 이전에 설명한 9개의 딥러닝 테스트로 구성되어 있다. 이들은 일반적으로 2가지 그룹으로 나눌 수 있다. 첫 번째 그룹에는 테스트 1, 2, 4, 5, 8, 9가 포함된다. 이들은 NNAPI를 완벽하게 지원하는 CNN 모델을 사용한다(즉, 사용된 TensorFlow 명령들은 Android 8.1에 소개된 NNAPI로 구현되어 있다.) 따라서 이들은 적절한 칩셋과 드라이버를 통해 하드웨어 가속을 사용해서 동작할 수 있다. NNAPI는 항상 시스템이 사용 가능한 AI 가속을 탐지하지 못하거나 CPU에서 모든 연산이 이루어지는 상황을 피하게 할 수 있다. 첫 번째 테스트는 양자화된 CNN 모델을 정량화된 CNN 모델을 실행하고 가속된 INT8 기반 연산의 성능을 확인하는 데 사용된다는 것을 언급해야 한다.  

두 번째 그룹은 다른 세 테스트를 포함하고(3, 6, 7) 이들은 신경망이 오직 CPU에서만 동작한다. 이들은 적어도 NNAPI가 아직 지원하지 않는 TensorFlow 명령을 하나는 포함하고, 현재는 작업을 위한 부분적인 가속을 사용하는것이 불가능하다. 이 테스트들은 고성능 컴퓨팅 및 이미지 처리를 위해 설계된 Arm NEON 명령어 셋을 CPU 기반 실행시의 속도와 퍼포먼스를 검증하기 위해 추가되었다. NNAPI 드라이버가 존재하지 않는 경우에, 첫번째 그룹의 테스트들의 모든 연산 또한 CPU에서 같은 명령어 셋을 사용하여 동작한다.   

테스트에서 사용된 이미지의 해상도는 최소 2GB RAM을 가지는 장치들과 1GB RAM이 있는 대부분의 장치에서 모든 테스트를 실행하기 충분한 메모리가 있도록 선택되었다. 테스트는 신경망이 적어도 하나의 이미지를 시간 내에 성공적으로 처리하면 통과할 수 있게 구성되어 있다. 특히, 1GB RAM을 가지는 장치(Galaxy S2/S3 mini, HTC One X, Fiio X7 etc.)를 테스트 하는 동안, 새로 재시작후 모든 모델을 실행할 수 있었다.  

처음 8번의 테스트는 각각 미리 지정된 시간 제한을 가진다: 25, 40, 40, 30, 40, 50, 20, 25 초. 마지막 테스트는 시간제한이 없다 - 이미지 품질 향상은 장치의 메모리가 부족해 질 때 까지 처리된다. 각 테스트의 실행 시간은 정해진 시간 내에 처리된 이미지들에 대해 평균으로 계산된다. 3개 이상의 이미지가 처리될 때, 처음 두 이미지는 신경망 초기화와 메모리 할당으로 추가적인 시간이 걸릴 수 있으므로, 처리 시간을 고려하지 않는다. 처음 8개 테스트의 점수는 평균 실행시간에 반비례하게 계산된다; 메모리 테스트의 점수는 해당 신경망이 처리할 수 있는 이미지의 최대 크기에 비례한다. 최종 AI 점수(그림 8)는 해당 아홉개의 테스트로 얻은 점수의 합계로 얻어지고, 특정한 기기의 AI 성능의 집계를 나타낸다. 이 테스트의 가중치 계수는 Android P를 사용하며, NNAPI가 없는 구글 Pixel 2에서의 테스트 결과를 기반으로 보정되었다.   

## 5. 벤치마크 결과
이 단락에서는, 벤치마크 결과를 10,000개 이상의 모바일 장치의 실제 환경에서 테스트 한 정량적 벤치마크 결과를 제시한다. 각 장치/SoC의 점수는 표 2, 표 3에 나타난다. 이는 각각의 테스트/신경망에서의 한 이미지당 평균 처리 시간, SRCNN 모델에서 최대 처리가능 이미지 해상도 그리고 집계된 전체 AI 점수를 보여준다. 이 점수는 해당 장치/SoC에서 수집한 특이치를 제외한 모든 결과의 평균으로 계산되었다. 자세한 설명은 아래에 있다.

### 5.1 신경망
표 1은 벤치마크에 포함된 딥러닝 아키텍처 세부사항의 요약이다. 표 2, 3의 결과는 메모리 소비와 상대적인 처리 시간의 이론적인 기대와 꽤나 일치한다. 특히, 첫 번째 테스트의 양자화된 MobileNet CNN은 3~4배 같은 float 모델에 비해 적은 RAM을 요구하고, CPU에서의 속도는 Inception-V3 CNN보다 훨씬 빠르다. 세 번째 얼굴 인식 테스트는 두 번째 얼굴 인식 테스트보다 두배 큰 영역의 이미지를 처리하고, 약 두배의 추론시간을 보인다. 이는 Inception-ResNet-V1과 Inception-V3의 성능이 유사하다는 것을 의미한다. 이미지를 이미지로 처리하는 작업에서 가장 효과적인 모델은 ICNet인데 이는 계산이 주로 축소된 이미지/특성 지도에서 이루어지기 때문이다. 비슷한 접근이 SRGAN 모델에서 이루어졌는데 이는 원본 이미지를 128 X 128 픽셀로 다운샘플링하여 원래 크기로 업스케일링하는 마지막 두 레이어 전까지 해당 해상도로 처리하는 모델이다. 그러므로 12개의 residual 블록을 사용하고, 512 X 512px 이미지를 사용할 때 업스케일링과 다운스케일링시에 요구하는 RAM이 높을 지라도 처리 시간은 여전히 합리적으로 유지된다. 이미지 향상 작업의 DPET 모델은 4개의 residual 블록을 포함하고, 다운샘플링 없이 이미지를 처리하고, 이로 인해 처리 시간은 이전의 경우보다 2배 빨라야하고, 실제로 그렇게 나타난다. 5번째 테스트의 VGG 19-모델은 사용된 모델중 가장 큰 자원을 소모하는데, 이는 19개의 convolutional layer로 구성되어 있고, 이는 이론적으로 DPED 모델보다 1.6배 느려야 한다(convolutional layer의 크기가 비슷하기 때문에). RAM 소모는 주로 가장 큰 convolutional layer의 치수로 결정되기 때문에, 비슷해야한다. 마지막으로, SRCNN모델은 VGG-19, DPED 두 모델보다 더 빠르고, 앞선 이유들로 인해 메모리 소모는 더 작다. SRCNN에서 처리할 수 있는 가장 큰 이미지의 해상도는 기기 자체의 RAM 용량의 증가와 평행하게 증가하고있다. 그러나 NNAPI 내부의 버그로 인해 안드로이드 8.1 이상의 버전을 사용하는 장치에서는 일반적으로 더 많은 RAM을 사용하고, 이 사실은 적용되지 않는다. 또한 지금까지의 결론은 하드웨어 가속을 지원하지 않는 기기를 기반으로 한 것이고, 따라서 NNAPI를 사용할 수 있는 1, 2, 4, 5, 8, 9에서의 결과는 크게 달라질 수 있다.   

### 5.2 스마트폰과 모바일 칩셋
표 2, 3는 몇몇의 선택된 안드로이드 스마트폰과 칩셋이 AI Benchmark를 통해 얻은 퍼포먼스를 나타낸다; 실제 실험의 전체 결과는 프로젝트 웹사이트 [http://ai-benchmark.com](http://ai-benchmark.com)에 있다. 상세 내용을 설명하기 전에, 먼저 표에 나타나는 결과에 영향을 미치는 몇몇 NNAPI 버그에 대해 언급한다. 먼저, Android 8.1 NNAPI 드라이버 때문에, (convolutional) 명령의 퍼포먼스는 드라이버가 비활성화 되었을 때 보다 두배 느리다. 그러므로 표 3에서 나타내는 다양한 SoC의 실행시간 평균 연산에서 해당 문제를 나타내는 전화기를 제외했다. 화웨이의 Android 8.1 이상 안드로이드를 사용하는 전화기와 Kirin 970 칩셋은 그들만의 커스텀된 NNAPI를 사용했지만, 여전히 다른 버그를 겪었다 - 긴 대기시간 이후, Kirin NPU의 클럭 속도 하락이 휴대폰이 재시작될 때 까지 돌아오지 않는다. 두개의 표 모두에서 화웨이의 실험 결과는 재부팅 직후에 실험한 결과이다. 마지막으로, Android NNAPI 를 사용하는 장치의 RAM 소모가 이미지를 이미지로 처리하는 테스트에서 2배 이상 높게 나타났다. 이는 3.2 단락에서 언급한 ByteBuffer 이슈로, 마지막 메모리 테스트에서 결과가 나타난다.   

아래는 SoC 제조사별 결과를 정리하고, 시중의 칩셋 성능을 설명한다. 

* __Qualcomm.__  
Snapdragon 칩셋은 이제 양자화된 신경망을 위한 하드웨어 가속을 제공한다(퀄컴의 NNAPI 드라이버가 존재할 때), 하지만 일반 모델은 아직 기존 상용 장치에서 지원하지 않는다. 이 드라이버를 탑재한 첫 번째 스마트폰은 OnePlus 6이고, SnapDragon 845 SoC를 사용하며, 안드로이드 P 펌웨어를 사용하는 최신 모델이다. 이는 양자화된 MobileNet 모델을 25ms 이내에 Hexagon DSP에서 가동할 수 있다. 이는 상응하는 CPU 속도(60-65ms)에 비해 상당히 빠르다. 비슷한 퍼포먼스는 똑같은 Hexagon 685 DSP를 탑재하는 Snapdragon 670/710칩셋에서도 기대할 수 있다; 퀄컴의68X 시리즈를 탑재한 Hexagon 682를 탑재한 Snapdragon 835와 Hexagon 680을 탑재한 Snapdragon 636/660/820/821는 런타임이 조금 더 길어야 한다.  
퀄컴의 float 모델의 가속을 지원하는 NNAPI 드라이버에 대한 공식적인 테스트가 없지만, 이 드라이버의 베타 버전을 (아마도)사용하고 Adreno 506 GPU가 통합된  Snapdragon 625 Soc는 CPU 기반의 실행과 비교했을 때 2배의 속도 증가를 제공할 수 있다. Adreno 506의 퍼포먼스는 130 GFLOPS 정도인데, 이는 Snapdragon 845의 Adreno 630 (727 GFLOPs)는 3~4배의 속도 증가를 제공할 수 있다는 것을 의미하지만, 정확한 숫자는 크게 다를 수 있다.   
행렬/딥러닝 연산 관련 CPU 성능 측정에서, 현재 가장 우수한 성능을 보이는 퀄컴 코어는 Snapdragon 845 SoC에 있는 Kryo 385 Gold 이다. 이는 Snapdragon 835의 Kyro 280 코어보다 30% 가량 향상되었다. 재미있게도, 후자는 non-Cortex 기반으로 설계된 Snapdragon 820 SoC의 Kryo 첫 세대와 비교했을 때 유사하거나 약간 낮은 성능을 보인다. 이는 4개의 코어를 가짐에도 불구하고, 새로운 Kryo 260 코어를 가지는 Snapdragon 636/660보다 약간 더 빠르다. Krait 마이크로아키텍처를 대표하는 2013년도의 Snapdragon 800/801은 여전히 경쟁력있는 결과를 보여준다. 이는 2xx, 4xx, 6xx 시리즈의 주요 SoC, 심지어 그 이후 출시된 Cortex-A57 마이크로아키텍처를 기반으로 하는 810과 808 칩셋을 능가한다. 우리는 또한 커스텀된 퀄컴 CPU 코어가 보통 기본 Arm Cortex 아키텍처보다 나은 성능을 보여준다는 사실을 주목한다. 

* __Huawei.__ 
하이실리콘 Soc의 CPU 성능은 Qualcomm만큼 인상적이지 않더라도, NPU가 포함된 Kirin 970은 float 딥러닝 모델에서 환상적인 속도 증가를 보여준다. 특히, 작업에 따라서는 CPU와 비교했을 때 7-21배 빠른 추론을 보여주기도 하고, 전반적인 CPU의 최고 성능에 비해 4-7배 높은 성능을 보여준다. 하드웨어 가속을 지원하는 테스트 2, 4, 5, 8에서 이는 각각 하나의 이미지를 처리 하는데 평균 132, 274, 240, 193ms를 필요로 했다. NPU의 유일한 단점은 양자화된 모델에 대한 가속을 지원하지 않는다는 것이다 - 첫 번째 테스트에서 모든 연산은 CPU에서 연산되고, 이미지당 평균 처리 시간은 160ms가 걸렸다. 이는 상응하는 DSP가 활성화된 Snapdragon 845 모델에 비해 상당히 높은 결과이다. 이 문제는 Kirin의 NNAPI에서  양자화 모드를 구현하면 해결되지만, 현재는 이 기능이 아직 개발되지 않았다.   
다른 하이실리콘 칩셋들은 AI 어플리케이션을 위한 가속을 지원하지 않고, 모든 연산은 CPU에서 실행된다. 하이실리콘의 모든 SoC들은 Arm Cortex 코어를 기반으로 하고, 그들의 성능은 다른 Cortex 구조의 칩셋들과 상당히 유사하다.    

* __MediaTek.__ 
Helio P60은 양자화 모델과 float 모델 모두의 가속을 지원하는 NNAPI 드라이버를 보유한 최초의 칩셋이다. 양자화된 모델은 통합된 APU에서 동작하고, 이는 Hexagon 685 DSP와 유사한 성능을 보여준다 - 첫 번째 테스트에서 하나의 이미지를 처리하는 데 21ms 가 걸린다. float 모델은 Mali-G72 MP3 GPU에서 실행된다. 이는 CPU와 비교했을 때 2-5배 의 가속을 지원하고, CPU의 가장 좋은 결과에 비해 1.5-2x 빠른 실행시간을 가진다. 이 수치들은 미디어테크의 개발자 전화기에서 얻어졌고, Helio P60 기반의 NNAPI 드라이버(Vivo V11)을 가진 실제 기기는 약간 더 나쁜 성능을 보여준다.   
다른 미디어테크 칩셋은 현재 AI 어플리케이션을 위한 가속을 지원하지 않는다. 그들은 기본 Arm Cortex 구조를 가진 CPU코어에서 작동한다.  

* __Samsung.__
논문을 작성하는 당시, 삼성의 SoC는 써드파티 AI 어플리케이션을 위한 가속을 지원하지 않았다: 이들의 칩셋을 가진 모든 장치는 기본 NNAPI 드라이버를 사용했다. 가장 최근의 Exynos 9810 칩셋은 미디어테크의 P60 칩셋과 같은 Mali-G72 그래픽을 가진다.(하지만 코어의 갯수는 3개 대신 12개이다). 따라서 우리는  Arm NN 라이브러리가 삼성의 NNAPI 드라이버에 통합될 경우, float 모델에서 3-4배의 속도 증가를 기대할 수 있다. 최근의 삼성 Exynos 프로세서는 Arm Mali GPU를 사용하기 때문에, 같은 로직이 그들에게 적용될 수 있다.   
작업에 따라, 삼성의 Mongoose M3 CPU 코어는 Snapdragon 845에 포함된 Kyro 385 코어와 비교했을 때 상당히 좋거나 상당히 나쁜 퍼포먼스를 보여준다.하지만 그들의 전체적인 퍼포먼스는 상당히 비교해 볼 만 하다고 여겨진다. Mongoose M2 마이크로아키텍처는 M1 버전에 비해 50%나 향상되었고, M2와 M3의 퍼포먼스 또한 비슷하다. Exynos 8895와 9810 SoC에 주목할 만한 이슈는 CPU 성능을 관리하는  전력 관리 시스템이다. 이는 매우 불안정한 결과를 유발한다: 특히, 여러 벤치마크를 후속 실행할 때("고사양 모드"에서 10분 간격으로) Galaxy S9는 총 점수에서 50%의 변동을 나타냈다. 다른 장치에서 얻은 결과는 훨씬 더 큰 변동폭을 나타냈다(예를 들어, 7번째 테스트에서 200-800 ms의 결과를 나타냈다). 현재는 성능 모드가 통합적인 로직에 따라 동작하기 때문에 외부에서 조정할 방법이 없다.  

* __Others.__ 
널리 사용되지 않거나(Spreadtrum), 생산 중단된 칩셋에서도(인텔 Atom, Nvidia Tegra, TI OMAP) 결과를 얻었다. AI와 딥러닝의 맥락에서 흥미로운 것은 Nvidia Tegra 플랫폼인데, 이는 딥러닝을 위한 기초 요소의 GPU 가속 라이브러리 CUDA와 cuDNN를 지원한다. 불행히도, 2015년 이후 Nvidia SoC를 사용하는 새로운 기기가 없다. 또한 기존의 것은 이미 단종되었고, 머신러닝 모바일 프레임워크 가속을 위한 NNAPI 드라이버를 지원받지 못한다.   

## 6. 고찰
모바일 장치에서의 머신러닝을 지원하는 소프트웨어와 하드웨어는 이제 몇 개월마다 발표되는 다양한 milestone과 함께 아주 빠르게 진화하고 있다. 그들이 새로운 가능성과 높은 레벨의 퍼포먼스를 확실히 가져오고 있지만, 현재 표준 요구사항과 공공적으로 사용 가능한 사양의 부족으로 실제 장점과 한계에 대한 객관적인 평가를 항상 허용하지는 않는다. 아래에서 우리는 우리의 하드웨어 가속을 지원하는 NNAPI 를 통해 모바일 머신러닝 프레임워크 및 칩셋으로 한 작업을 요약하고자 한다.   

현재, 안드로이드에서 딥러닝 사용을 시작하는 가장 쉬운 방법은 안정적이고 발달된 TF Mobile 프레임워크를 사용하는 것이다. 이는 2년 전에 소개되었고, 주요 이슈들은 이미 해결되었다. 작은 문제에 많은 정보는 다양한 웹사이트에서 사용 가능하다. 만약 하드웨어 가속이 주요한 문제라면, TF Lite는 여전히 하나의 방법이 될 수 있지만, MobileNet, Inception CNN을 이용한 이미지 분류 보다 복잡한 것을 위해 이를 사용하는 것은 추천하지 않는다. 이는 여전히 일부 모바일 플랫폼에서 표준화되지 않은 모델 아키텍처를 사용할 때 가끔 문제가 생길 수 있기 때문이다. TF Mobile 에서 TF Lite로 이주하는 것은 유사한 안드로이드 프로그래밍 인터페이스를 사용하기 때문에 비교적 쉽다(가장 큰 차이점은 학습 완료된 모델이 .pb 에서 .tflite로 변경되는 것이다). 또한 그러므로, TF Lite는 더 나은 지원을 받을 수 있다. 만일 어플리케이션이 특정 장치나 SoC를 목표로 한다면, 상응하는 독점 SDK 또한 사용할 수 있다. 하지만 이 경우, 개발이 쉽거나 편리하지 않을 수 있다. Caffe2 모바일과 다른 널리 사용되는 프레임워크에 관해서는, 그들의 커뮤니티는 지금은 매우 작으며, 이는 가이드가 없고, 인터넷에 문제의 해답이 없다는 것을 의미한다. 그러므로 나타나는 모든 문제는 주로 해당 프레임워크에 상응하는 깃허브에 이슈를 생성하는 것 만으로 해결될 것이다.   

안드로이드 장치에서의 AI 알고리즘을 위한 하드웨어 가속은 심지어 더 논란이 많은 주제이다. 이 논문을 쓸 때, 기존의 float 딥러닝 모델의 가장 빠른 실행시간은 화웨이의 Kirin 970 칩셋을 사용하는 기기였다. 이들은 발표 당시 시장에서 크게 앞서있었다. 그러나 우리의 분석에 따르면, 거의 모든 SoC 제조사가 그들의 새로운 칩셋에서 유사한 성능을 낼 수 있는 가능성이 있었기 때문에, 향후 관점에 대해서는 중립을 유지한다. 실제 상황은 다음 해가 시작되면 명확해 질것이다. Kirin 980을 사용하는 최초의 기기와 미디어테크의 P80, 퀄컴과 삼성의 다음 SoC가 시장에 나타날 것이기 때문이다. 성능 이외에도 우리는 또한 전력 효율을 함께 관측했는데, 상당한 배터리 소모는 몇몇 표준 카메라 내부 처리 기술을 사용하는 데에 제한이 생기기 때문이다.   

여기서 다루려고 하는 마지막 주제는 양자화된 모델의 사용이다. 이들의 현재 적용 범위는 다소 제한적이다. 이는 더 복잡할 뿐만 아니라, 이미지 분류를 위해 훈련된 모델의 양자화를 위한 신뢰할 수 있는 표준 도구가 없기 때문이다. 현재는 이 분야에서 두 가지 다른 발전 방식을 기대할 수 있다. 첫 번째는 양자화의 문제가 언젠가 크게 해결되고, 대부분의 스마트폰은 양자화된 신경망을 사용할 것이라는 것이다. 두 번째 경우는, 특정 NPU가 지원하는 float 모델은 더 강력해지고 효율적이게 되어, 연산 능력이 부족했던 많은 문제에 대해 양자화의 필요성이 사라지는 것이다. 우리는 미래를 쉽게 예언할 수 없음으로, 우리는 벤치마크에서 양자화와 float 모델의 혼합하여 사용했지만, 향후의 출시에서는 상응하는 비율은 상당히 변경될 수 있다.    

현재는 머신러닝 프레임워크에 관련된 하드웨어와 소프트웨어, 그리고 새로운 전용 칩셋만이 해결할 수 있는 많은 문제가 있기 때문에, 우리는 모바일 장치에서의 AI 가속의 실제 상태 벤치마크와 머신러닝 분야의 변경과, 상응하는 조정을 벤치마크에 반영하는 리포트를 출간하려고 한다. 가장 최근에 관측된 AI Benchmark와 실제 테스트에 대한 설명은 월별로 프로젝트 사이트인 [http://ai-benchmark.com](http://ai-benchmark.com)에 업데이트 될것이다. 추가적으로, 기술적인 문제나 추가적인 질문이 있는 경우, 해당 논문의 1, 2 저자에게 항상 접촉할 수 있다. 

## 7. 결론
이 논문에서, 우리는 안드로이드 분야에서의 AI와 머신러닝 분야의 가장 최근의 성취를 논의했다. 첫째, 우리는 현재 존재하는 스마트폰에서 딥러닝을 위한 하드웨어 가속을 위해 잠재적으로 사용될 수 있는 모든 모바일 칩셋의 개요를 제시하고, 모바일 장치에서 AI 알고리즘을 실행하는 데에 인기있는 프레임워크를 설명했다. 우리는 안드로이드 장치나 스마트폰에서 딥러닝을 실행하는 데 관련된 측면의 퍼포먼스를 측정하는 AI 벤치마크를 제시했다. 또 50개 이상의 다른 모바일 SoC와 10,000개 이상의 모바일 장치에서 이 벤치마크를 통한 실제 결과를 관측했다. 마지막으로 우리는 해당 분야에 관련된 미래의 소프트웨어와 하드웨어의 개발에 대해 논의하고, 안드로이드 장치에서 딥러닝 모델을 사용하는 개발에 대한 우리의 추천을 제시했다. 
