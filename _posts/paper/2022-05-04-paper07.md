---
title:  "Say Goodbye to Off-heap Caches! On-heap Caches Using Memory-Mapped I/O 번역"
excerpt:  "Memory-Mapped I/O 를 사용한 On-heap 캐시 논문 번역"
categories:
  - Papers
tags:
  - [android, papers]
toc: true
toc_sticky: true
sidebar: 
  nav: "docs"
---

## 논문 번역
NosLab 연구생 활동 기간동안 읽은 논문을 번역했다. 저작권을 염려하여 그림은 삽입하지 않았다.   
원 논문은 [https://www.usenix.org/conference/hotstorage20/presentation/kolokasis](https://www.usenix.org/conference/hotstorage20/presentation/kolokasis) 에서 찾아볼 수 있다.  
저자는 Iacovos G. Kolokasis 등이다. 

# Memory-Mapped I/O 를 사용한 On-heap 캐시 논문 번역
## Say Goodbye to Off-heap Caches! On-heap Caches Using Memory-Mapped I/O

## Abstract
많은 분석적 연산이 수렴 조건이 충족될 때 까지 실행되는 반복 처리 단계에 의해 지배된다.
느린 DRAM 용량 증가와 데이터가 기하급수적으로 증가할 때 워크로드를 가속하기 위해 Spark는 중간 결과의 off-memory caching을 사용한다. 
하지만, off-heap caching은 데이터셋의 크기가 증가할때 특히 상당한 오버헤드를 가하는 데이터의 직렬화/역직렬화(SerDes)를 필요로 한다.  

이 논문은 Spark 데이터 캐시의 확장으로, memory-mapped I/O를 통해 모든 cache 데이터를 on-heap이며 off-memory 상태로 저장함으로 serdes의 필요성을 피하는 TeraCache를 제안한다.
이를 이루기 위해, TeraCache는 memmory-mapped 된 고속 저장 장치에 상주하고 캐시된 데이터만 명시적으로 사용되는 managed heap을 통해 오리지널 JVM heap을 확장한다.
예비 결과는 TeraCache 프로토타입이 최신의 serdes 접근에 비해 중간 결과 약 37% 가량 Machine Learning 워크로드의 속도를 향상시킬 수 있는 것으로 나타났다. 

## 1. Introduction 
분석 어플리케이션들은 많은 양의 데이터를 처리하기 위해 종종 ML(머신 러닝) 알고리즘을 사용한다.
이런 어플리케이션들은 수렴 조건을 충족할 때 까지 한번 혹은 그 이상의 연산 과정을 반복한다.
이런 워크로드의 속도를 가속하기 위해 Spark는 복잡한 연산 파이프라인의 중간 결과를 caching하고 각 단계에서 재사용한다.
중간 결과는 RDDs(Reslient Distributed Datasets)로 LRU cache에 저장된다.  

그림 1은 세가지의 잘 알려진 ML 워크로드에서의 RDD caching의 성능을 보여준다 : Linear Regression(LR, 선형 회귀), Logistic Regresson(LgR, 로지스틱 회귀), Support-Vector Machines(SVM, 서포트 벡터 머신)
모든 워크로드는 64GB의 데이터셋과 32GB DRAM과 30개의 CPU 코어를 사용하는 단일 Spark Excutor에서 실행되었다(우리는 3회 실행한 평균값을 기록했다. 편차가 최소화되어 생략되었다).
메모리와 디스크 모두를 사용하는 환경에서(hybrid) RDDs Caching은 중간 결과를 모두 다시 계산하는 것에 비해 약 90%의 성능을 향상시켰다.     

DRAM에만 caching하는 것은 장기적인 해결책이 아닌데, 이는 DRAM 확장성이 한계에 도달하는 동안 생성되고 처리되는 데이터의 양이 높은 비율로 증가하기 때문이다. Cached된 데이터는 점진적으로 DRAM의 물리적 크기를 초과하고, 각 단계에서 중간 결과를 재연산하게 만들기 쉽다. 
그러므로 현재 관행은 Spark의 중간 RDDs를 저장하기 위한 cache 사이즈를 증가시키기 위해서 빠른 저장 장치를 사용하는 것이다.
SSD나 NVM 뿐만 아니라 NVMe 블록 장치같은 NAND flash storages는 DRAM보다 더 높은 용량과 밀도를 가진다.
DRAM은 각 DIMM 슬롯당 GB 규모인데 반해 SSD와 NVMe 장치들은 더 적은 비용으로 각 PCIe slot 당 terabytes의 규모를 가진다.
더욱이, NVMe bloc device는 NVM과 비교했을 때 적은 비용으로 더 높은 용량을 제공하며, 몇 마이크로초 정도의 접근 지연을 가지며 읽기 및 쓰기 모두 수백만 IOPS를 수행한다.  

실행 중에, Spark는 최초에 RDDs를 heap(DRAM)에 위치시킨다.
만일 하나의 RDD 블록이 메모리에 맞지 않으면 이는 off-heap(disk) 캐시로 직렬화되고 메모리는 다음 가비지 컬렉션에 회수된다.
유사하게, 만일 충분한 메모리가 존재하지 않으면 LRU 캐시는 오래된 항목을 off-heap 캐시로 퇴거시킨다.
Spark가 off-heap에 저장된 RDD를 참조할 때, 디스크에서 직렬화된 블록을 메모리로 역직렬화한다.
RDDs는 변경할 수 없기 때문에 모든 블록은 최대 한번 직렬화된다.
하지만, 각 블록마다 역직렬화는 반복적인 단계에서 여러 번 일어날 수 있다.  

오늘날, 뛰어난 재계산 전략에도 불구하고, off-heap 장치에 caching하는 것은 높은 serdes 오버헤드를 발생시킨다.
Zhang에 따르면 serdes가 디스크 I/O보다 오버헤드를 지배한다.
그림 1 (b)는 LR, LgR, SVM에서 RDDs를 off-heap에만 저장할 때 RDD cache의 성능을 보여준다.
off-heap caching과 serds의 오버헤드의 큰 영향을 주의해라.
serdes는 disk를 사용할 때의 전체 실행 시간의 평균 27%를 차지한다.
역직렬화 연산이 전체 serdes 연산의 80~90%를 차지하는데, 이는 워크로드가 변경할 수 없는 cached된 RDDs를 매 반복마다 장치에서 검색하기 때문이다.
Serdes 오버헤드는 저장 장치의 속도가 빨라지고 CPU와 메모리 성능의 격차가 좁혀질수록 악화되었다. 
전체 실행 시간 또한 주로 병렬 처리가 감소하고 디스크 처리량이 30 CPU 코어를 유지할 수 없기 때문에 CPU가 idle한 시간으로 인해 디스크에서만 caching할 때 증가되었다.   

on-heap cache(DRAM)를 함께 쓰는 방법은 serdes 비용을 줄여주지만, 여전히 GC 오버헤드가 존재한다. 
그림 1(b)는 현재 흔히 사용하는 방식인 RDDs를 하이브리드 cache에 저장할 때의 성능을 보여준다. 
비교적 큰 on-heap cache를 사용하는것은(Spark는 60%의 힙 공간을 캐시로 비축한다), 일부 RDDs를 메모리에 저장함으로 스토리지에 저장하는 것 보다 평균적으로 20% 정도의 상당한 serdes 오버헤드 감소를 보여준다.
하지만, 이런 큰 on-heap cache는 스토리지에만 저장할 때 보다 SVM에서 13배, LgR에서 36배 정도의 GC 시간을 증가시킨다.
Cached된 RDDs는 최초에 힙에 위치하고, 높은 비율의 수명이 긴 객체를 수명이 짧은 객체로 만든다.
따라서 GC는 JVM 힙에 살아있는 객체를 마킹하는데 시간을 더 소모하고, 힙의 큰 비율은 RDDs로 가득 차 있기 때문에 적은 비율의 메모리 회수를 끝낸다.
본질적으로, Spark는 실행과 cache 메모리를 위해 DRAM-only JVM 힙을 사용한다.
이것은 예측할 수 없는 성능 혹은 실패를 초래할 수 있는데, 큰 데이터를 캐싱하는 것은 실행 중에 추가 GC 부담을 초래하기 때문이다.   

이 연구에서 우리는 RDD caching이 메모리에서 고속 스토리지 장치에 매핑되며 GC되지 않는 힙의 일부분의 크고 관리되는 on-heap 캐시에서 수행되어야 한다고 주장한다.
TeraCache는 JVM 힙을 실행 부분과 cache 부분으로 분할하여 실행 heap은 DRAM에 위치시키고, cache부분은 DRAM-mapped block device에 위치시킨다. 
이는 본질적으로 serdes와 모든 관련된 오버헤드를 제거하고 cached된 데이터의 GC를 예방하며, 장치 기술 동향 및 서버 전력 제한과 일치한다.   

다음으로, 우리는 Spark에서 DRAM이 실행과 cache 사이에서 어떻게 나누어야 하는지를 관찰한다.
명백하게, 힙의 실행 부분은 작업 수행 중에 GC에 대한 부담을 유발하지 않기 위해 충분한 DRAM을 사용해야 한다.
반대로, cache 힙으로 DRAM을 많이 사용할수록, cached된 데이터에 더 빠른 접근이 가능하다. 
우리는 JVM이 실행과 caching을 모니터링하여 동적으로 DRAM의 사용을 조정하는 디자인을 제안한다.  

우리의 접근 방식 TeraCache는 on-heap RDD 캐싱과 memory-mapped 고속 스토리지 장치의 공동 설계 접근방식이다.
TeraCache는 Spark cache, JVM 메모리 관리, GC, MMIO(Memory Mapped I/O)에 걸쳐 있다. 
TeraCahce의 주요 이점은 다음과 같다:
 - 큰 memory-mapped 힙에 caching함으로 JVM이 cached된 데이터를 직접 load/store 할 수 있기 때문에 serdes의 필요성을 제거한다.  
 - cached된 RDDs를 유지함에도 JVM 힙을 분리하여 사용하기 때문에 GC의 빈도와 길이를 줄인다.  
 - MMIO 및 실행에 사용되는 DRAM을 동적으로 조정하여 실행 속도와 I/O 오버헤드의 균형을 조절한다.  

우리는 Spark의 caching을 목표로 하는 TeraCache의 초기 프로토타입을 구현했고, 반복적인 ML 워크로드에서 효용성을 확인하기 위해 DRAM의 동적 크기 재구성을 조사했다.
우리의 검증은 현재 최신의 하이브리드 접근 방법에 비해 37%의 성능 향상을 보여준다.   

## 2. TeraCache: Caching Over a Device Heap
Spark는 하나의 드라이버와 여러개의 executor 프로세스로 구성되어있다.
드라이버는 executor이 드라이버의 작업을 수행하는 동안 모든 명령을 생성하는 Spark 유저들로부터 실행되는 메인 프로세스이다. 
그림 2는 Spark가 excutor 메모리를 어떻게 두가지 논리적 파트로 나누는지를 나타낸다: (1) 실행 메모리, 연산 중 임시 데이터를 저장하기 위함, (2) 저장 메모리, 중간 RDDs를 caching하기 위함(LRU 캐시, Least Recently Used).
각 executor는 JVM 인스턴스에서 실행되고 DRAM에 위치하는 JVM 힙의 메모리를 할당받는다.
하나의 RDD가 storage 메모리에 맞지 않을 때 직렬화되어 디스크로 보내진다.   

우리의 연구는 executor 메모리의 연산, caching 두 가지를 사용한다.
우리는 물리적으로 JVM 힙을 이 두 가지 규칙에 따라 분할했다.
그리고, 우리는 JVM 힙의 각 부분을 메모리 계층 내부의 자원을 특정하기위해 그림 2(b)와 같이 매핑했다: (1) JVM Heap(H1)는 독점적으로 DRAM(DR1)에 할당되고 generator에 의해 분할될 수 있다(예: New, Old) (2) cached된 RDDs 모두를 저장하고, 크기는 장치의 용량에 따라 제한되는 커스텀 managed heap(TeraCache Heap)  

mmap에서 사용되는 페이지를 위한 메모리 매핑은 DRAM(DR2)의 남아있는 부분에 위치한다. DR1과 DR2는 실행 중에 적응 정책에 따라 TeraCache를 통해 동적으로 조정된다.
이를 위해 TeraCache는 아래에서 묘사한 것과 같이 Spark Block Manager, JVM의 가비지 컬렉터의 여러 확장을 요구한다.  

### 2.1 Design Challenges
**NVMe에서 mmap을 이용한 TeraCache 힙 할당은 현실적이다:** 우리는 JVM을 추가적인 힙을 할당하도록 수정했고, 메모리 매핑을 고속 스토리지에 memory-mapped 되도록 수정했다(예를 들어 NAND-Flash SSD or NVMe에 Linux mmap 를 이용하여).
고속 SSD나 NVMe 장치는 HDD와 반대로 생산되는 접근 패턴과 장치의 특성 때문에 mmio를 받을 수 있다.
그림 3(왼쪽)은 LgR을 HDD와 NVMe에서 serdes와 mmap을 수행했을 때의 성능을 보여준다.
두 가지 경우 모두 우리는 18GB의 데이터셋을 사용했고 8GB DRAM과 30개의 코어에서 하나의 Spark executor을 사용했다.
실제 cached 되어야 하는 훈련 set은 DRAM 캐시 크기의 10배이다.
평균 요청 크기를 보여주는 그림 3(오른쪽)에 나타나듯, mmap은 작고(page 크기 까지 (4KB)) serdes에 비해 비교적 랜덤한 I/O를 제공한다.
HDD는 이 접근 패턴에서 잘 작동하지 않는다. 
3배 큰 request 크기를 가진 serdes는 더 큰 serdes CPU 오버헤드를 가짐에도 불구하고 mmap보다 3배 더 낫다.
하지만 NVMe는 작은 request 크기에 대해 접근 패턴과 관계 없이 높은 처치량과 낮은 지연을 달성하여 serdes에 비해 mmap 36% 나은 성능을 나타냈다.
우리는 FastMap과 같은 최적화된 mmio를 사용함으로 성능을 더 향상시킬수 있을 것이라 기대한다.  

serdes를 피하기 위해 JVM의 힙을 키우는 다른 방법은 OS 가상 메모리 시스템을 사용하여 NVMe를 스왑 공간으로 사용하는 것이다.
비록 이것은 NVMe에 매우 큰 JVM 힙을 저장할 수 있더라도, 이는 RDD 캐시 오브젝트 목표로 단독으로 사용될 수 없고, 또한 GC를 피할 수 없다.
TeraCache가 실행과 caching을 위한 두 분리된 힙들을 사용하기 때문에, 명시적으로 cache의 GC를 피하고 저장장치에 cache 하기 위해 mmap은 더 적합한 메커니즘이다.
그림 4(a)는 힙의 실행 및 캐싱 부분에 대해 별도의 힙을 유지 관리하고, 캐싱에 NVMe를 엄격하게 사용하고, 캐시에서 GC를 방지하는 것이 모두 성능에 중요함을 보여준다.
TeraCache는 간단하게 큰 단위로 GC를 하면서 단독 JVM을 쓸 때 보다 최대 2배 향상을 보였다.  

**TeraCache 힙 관리는 비용이 큰 GC를 피한다:** 
JVM은 실행-저장 메모리 분리를 인식하지 못하기 때문에, 모든 객체는 JVM 힙 공간에 할당된다(그림 2(a)의 중간 레이어).
이는 다음과 같은 이유로 GC 시간을 증가시킨다: (1) cached 된 RDDs가 오랜 수명을 가진 객체의 모음이고, Spark 응용 프로그램의 명시적 지속 및 비지속 작업으로 관리된다.
그러므로, 그들은 GC에 드물게 수집되고, 개체를 탐색하는데 상당한 시간이 걸린다; 그리고, (2) 실행 메모리에 충분한 회수를 위해 더 많은 GC 사이클을 요구하는데, 이는 수명이 긴 RDD로 인해 각 GC 사이클이 적은 양의 메모리를 회수하기 때문이다. 
cached 된 객체의 생명 주기는 Spark 캐시에 얼마나 오래 유지되는지에 따라 명백하게 정의되어, 우리는 TeraCache에서 GC를 피할 수 있다.
따라서 우리의 디자인은 TeraCache를 관리하기 위해 커스텀 allocator를 사용하고, 다음과 같이 객체 회수를 줄인다.  

우리는 가비자 컬렉터를 증강하고 H1과 TeraCache의 차이를 인지시켰다. 
H1은 기본적인 JVM 힙으로 처리되고 일반적인 GC 알고리즘을 사용하여 수집된다. 
TeraCache는 커스텀한 지역 기반  allocator를 사용하고, 자바 메모리 모델을 준수한다.
특히, 우리는 TeraCache를 RDDs에 상응하는 지역으로 구성했다.
따라서, TeraCache는 Broom과 비슷하게 동일한 지역에 동일한 생명 주기를 갖는 객체를 배치단위로 한번에 free 할 수 있다. 
메모리 안전성을 유지하기 위해, 우리는 TeraCahce에서 H1을 참조할 수 없게 했다.
우리는 각 RDD 객체와 이 객체에서 도달할 수 있는 모든 객체를 TeraCache로 이주하여 이를 해결했다.
그러면, GC는 H1의 유효한 객체를 찾기 위해 TeraCache를 횡단할 필요가 없다.
RDDs는 항상 다른 객체를 오염시키지 않고 변하지 않고 안전하게 TeraCache로 이동할 수 있다.  

**RDDs를 TeraCache 에 Caching하는것:** 
RDD caching은 명시적으로 개발자에 의해 Spark RDD API를 통하여 관리됨으로, 우리는 Spark Block Manager상응하는 코드 포인트에 주석을 달기 위한 두 가지 Java 에노테이션  @cache와 @uncache 를 소개한다.
Block Manager는 caching, 직렬화, 데이터 전송 등을 관리하는 Spark 구성 요소이다.
우리가 소개하는 에노테이션은 RDD가 Spark에 의해 캐시되거나 캐시되지 않음을 JVM과 통신하는 구문 메타데이터이다.
@cache 에노테이션은 RDD가 cacahing 될 것을 암시하고, TeraCache는 mark-sweep GC의 마킹 과정과 유사하게 RDD 데이터 순회를 수행하며 도달할 수 있는 모든 객체를 마킹하고 적절한 TeraCache 구역으로 이주시킨다.
어플리케이션이 Spark Block Manager에게 cache를 요청할 때 RDD객체는 이미 생성되어있기 때문에 우리는 데이터를 직접 할당하기보다 H1에서 TeraCache로 이동한다.
각각, @uncache 에노테이션에서, JVM은 TeraCache로 부터 RDD 블록과 그 공간을 회수할 수 있다.
추가로, TeraCache가 용량의 한계에 도달했을 때 RDDs를 회수하기 위해 RDD의 퇴거를 처리하는(LRU 정책을 기반으로 메모리가 가득 차면) Spark Block Manager 함수를 @uncache로 사용자에게 api에 대한 주성글 추가했다.  

**DR1과 DR2 사이의 DRAM의 분할:** 
그림 2는 TeraCache가 DRAM을 다음 두 부분으로 나우는것을 나타낸다: (1) DR1은 H1을 위해 사용되고, (2) DR2는 TeraCache의 memory mapping을 위한 cache로 사용된다.
작업 수행 중에 GC로 소모되는 시간을 줄이기 위해, DR1은 새로 생성된 객체를 가능한 많이 수용할 수 있을 만큼 충분히 커야 한다.
동시에, DR2의 크기는 mmap I/O를 유발하는 page fault의 수를 결정하고, 이는 cached 된 데이터에 대한 평균 접근 시간에 직접적으로 영향을 미친다.
이상적으로, 우리는 작업이 새로운 객체를 생성할때를 위해 그리고 memory mapping을 위해 충분한 공간이 필요하다. 
하지만, DR1과 DR2가 물리적으로 DRAM을 공유하기 때문에, DR1이 클수록 DR2가 작고 반대도 동일하다.
그러므로, 우리는 동적으로 DR1과 DR2의 크기를 조정하는 방법을 제안한다.  

우리는 크기 조정의 필요 여부를 결정하기 위해 두 metric을 사용한다.
DR1을 위해, 우리는 minor GCs/s의 빈도를 측정하는데, 빈번한 GC는 작업이 더 많은 메모리 공간(DR1)을 필요로 한다는 것을 나타내기 때문이다.
유사하게, DR2를 위해 single memory-map의 pagefault 비율을 측정하는데, 이는 높은 비율의 page fault는 memory mapping을 위한 더 많은 공간이 필요함을 의미하기 때문이다. 
전반적으로, pagefaults의 비율이 상승하고 minor GC의 빈도가 낮으면, 우리는 DR1의 크기를 줄이고, DR2의 크기를 증가시키고 반대의 경우도 마찬가지이다. 
Section 3은 DR1과 DR2의 동적 크기 조정의 필요성을 보여준다.   

### 2.2 Prototype Implementation
우리는 ParallelGC를 기반으로 TeraCache의 초기 프로토타입을 구현했다.
ParallelGC는 JVM 힙을 두 세대로 분할한다: (1) 수명이 짧은 객체를 위한 NewGen, 그리고 (2) 수명이 긴 객체를 위한 OldGen.
NewGen은 하나의 Eden 공간 그리고 동일하게 분할된 두개의 survivor 공간들로 분할된다.
새로운 객체는 Eden 공간에 할당되고, minorGC에서 살아남은 객체는 survivor 공간으로 이동된다.
마지막으로 충분한 GC에서 살아남고 미리 정의된 거주 임계점에 도달한 객체는 OldGen으로 이동된다.  

우리의 프로토타입에서, 우리는 NewGen을 DRAM(H1)에 배치했고 mmap OldGen을 NVMe 장치에 배치했다. 
이 구현은 OldGen을 긴 수명을 가진 객체를 포함한 cached 데이터를 포함하는 cache로 사용한다.
Spark Block Manager는 JVM에게 cache 명령이 수행됨을 알려주지 않는데, 어쨌거나 가비지 컬렉터는 여러번의 minor GC 수행 후에 OldGen에서 cached 된 데이터를 승격한다. 
cached 된 데이터에서 GC를 피하기 위해, 우리는 명시적으로 OldGen 에서 GC를 삭제했다.   

우리의 프로토타입은 caching만을 목표로 하고, uncache 명령 중에 cached 된 RDDs를 회수하는 것은 지원하지 않는다. 
우리의 프로토타입은 캐시된 데이터 이외의 장수명 데이터를 Old Gen에서도 슬립할 수 있도록 한다.
OldGen이 확실히 주로 cached 데이터를 포함하고 약간의 uncached 데이터를 가지도록 하기 위해 우리는 GC의 tenured threshhold를 25로 지정했다.
이 threshold를 사용함으로 OldGen에 cache와 상관없는 데이터를 할당하는 것을 막았다. 
결과적으로 OldGen에 5%의 무관한 수명이 긴 데이터가 할당되었다. 
이를 통해 GC 시간과 I/O 트래픽을 평가할 수 있고, 이는 TeraCache의 더 완벽한 프로토타입에 의히 달성될 수 있기 때문이다. 

## 3. Evaluation
TeraCache 프로토타입 구현을 사용하여 우리는 다음과 같은 평가를 진행했다: (1) TeraCache와 하이브리드(기본) 사이의 전체적인 성능적 이점 (2) TeraCache 사용이 GC 오버헤드에 미치는 영향, 그리고 마지막으로 (3) DRAM을 DR1과 DR2로 분리한 것의 영향.

**실험 환경:**우리는 Intel(R) Xen(R) E5-2630 v3 CPUs(2.4GHz, 8개의 물리적 코어, 16개의 hyper-thread, 총 32개의 thread), 그리고 32GB의 DDR4 DRAM, CentOs v7.3, Linux kernel 4.15.72를 사용하는 듀얼 소켓 서버에서 실험을 진행했다. 
저장 장치는 PCIe가 탑재된 Samsung SSD 970 PRO 500GB를 사용했다.
우리의 실험에서, 우리는 OpenJDK v8u250-b70과 Spark v2.3.0을 사용했고, Spark executor를 30개의 쓰레드에서 실행했다.
우리는 TeraCache와 대항하여 하이브리드 방식의 KMeans(KM), LR, LgR 그리고 SVM 워크로드를 Spark-Bench Suite에서 검증하였다. 
각 워크로드는 64GB 데이터셋에서 100번의 반복을 실행했다.
또한, TeraCache의 경우 표 1에 나와 있는 다양한 구성으로 각 워크로드를 실행했지만, 하이브리드의 경우 힙 총 힙 공간의 60%를 활용하는 32GB 힙을 사용하고 전체 스토리지 디바이스를 off-heap RDD 캐시로 사용했다.  

**TeraCache를 사용하는 성능적 이점:**
그림 4는 각 벤치마크를 하이브리드(가운데)와 TeraCache(오른쪽)를 사용할 때의 전체 실행 시간을 보여준다.
TeraCache의 경우, 표 1에 상응하는 워크로드에서 성능이 가장 좋은 구성을 표기했다(예를 들어 LR과 LgR를 위한 구성 C, KM 과 SVF를 위한 구성 D).
하이브리드의 경우 우리는 더 많은 RDD를 on-heap 상태로 만들어 eviction을 최소화 하기 위해 가장 큰 힙사이즈를 사용했다.
우리는 TeraCache가 하이브리드와 비교해 전체 성능을 KM, LR, LgR, SVM에서 각각 7%, 32%, 37% 그리고 20% 향상시키는 것을 관측했다.
더 나은 이해를 위해, 실행 시간을 GC 시간과 Serdes + I/O, 기타 실행 시간으로 구분한다.  

**GC 오버헤드에서 TeraCache의 성능:**
Section 2에서 논의한 바와 같이, 그림 4는 LR, LgR, SVM 에서 각각 43%, 50%, 45%의 GC 시간이 감소함을 보여준다.
시간 감소의 원인은 TeraCache가 cached RDD를 마크할 필요가 없기 때문이고 또, TeraCache가 DRAM 힙을 일시적인 객체를 위해서만 사용하여 GC를 적게 수행하기 때문이다.
반면, 기존 Spark는 RDDs와 수명이 짧은 객체 둘 다 DRAM 힙에 보관하고, 직렬화된 RDD만 eviction 한다. 
구체적으로, KM에서 우리는 5%의 GC 시간이 증가함을 발견했다. 
우리는 KM에서의 이러한 증가를 작업이 큰 cached 데이터에 접근하지 않는 대신, H1에 많은 양의 셔플 데이터를 생성하기 때문이라 생각한다.
H1의 작은 크기는(그림 4 (c) A, B) GC의 부담을 증가시키고 그리고 빈번한 GC를 유발한다.
마지막으로, 심지어 GC 시간이 가장 짧은  그림 4(C)의 E에서도 아래에서 논의한것 처럼 많은 양의 page fault가 발생하기 때문에 실행 시간은 최소가 아니다.  

**DR1과 DR2 사이의 DRAM 분할의 효과**
Section 2에서 논의한 것 처럼, DRAM 분할은 전체적인 어플리케이션들의 성능에 영향을 미친다.
그림 4 (b), (c) 그리고 (d)는 표 1에 나타난 각 워크로드의 실행 시간, GC 시간, 전체 page fault의 수를 나타낸다.
KM은 D에서 최소의 page fault를 발생시켰지만, GC 시간이 E보다 16% 길었다.
KM은 단계 사이에 데이터 셔플을 실행하며, B와 C 사이의 총 실행 시간이 크게 개선된 것 처럼 단계 간에 셔플 데이터를 생성함으로 실행 메모리를 위한 공간이 더 많이 필요하다.
LR,과 LgR은 C를 SVM은 D를 사용할때 가장 적은 page fault와 가장 짧은 GC와 실행 시간을 가진다. 
반대로, LR과 LgR은 cached된 데이터를 빈번하게 접근하고, 따라서 mmap page를 위해 사용가능한 DRAM을 증가시키는데 더 잘 응답한다.
일반적으로, 구성별로 다양한 실행 시간은 어플리케이션의 패턴에 기인한다. 
캐시된 데이터에 대한 더 많은 액세스가 있는 벤치마크 단계는 DR2의 더 많은 mmap 페이지에서 더 많은 이점을 얻는 반면, 더 많은 임시 객체를 생성하는 단계는 H1을 증가시키면 이점을 얻는다.
따라서 DR1과 DR2를 실행 동안 크기를 재조정하는것은 이점이 있다.  

## 4. Discussion
TeraCache는 본질적으로 JVM 기반 데이터 처리 프레임워크임으로, annotation 알림을 인식하는 수정된 JVM을 필요로 한다.
TeraCache는 다음과 같은 속성을 갖는 어플리케이션에서 이점을 갖는다:  

 - 그들은 비슷하고 주로 수명이 긴 집합으로 묶일 수 있는 객체를 생성한다. 예를 들어 반복적인 연산을 수행하는 Spark 워크플로우는 축적된 데이터나 셔플 데이터와 같은 수명이 긴 객체를 사용한다. 이 객체의 그룹은 TeraCache가 그것의 TeraCache 힙에서 광범위한 GC 없이 지역을 free할 수 있게 한다. 우리의 초기 TeraCache 프로토타입은 해당 객체들을 TeraCache 힙으로 옮기기 위해 runtime 시스템을 기반으로 한다. 즉, 사용자와 프로그램에게 투명한 Spark caching 코드에 주석을 다는것. 
 - 그들은 전이적 폐쇄가 전체 힙을 포함하지 않는 객체를 생성한다. 보안성을 위해, 우리는 참조의 전이적 폐쇠(즉, 도달가능한 모든 개체)를 계산하고 TeraCache의 단일 영역으로 이동시켜서 TeraCache에서 H1으로의 포인터를 피했다. 이는 이러한 객체의 전이적 폐쇄가 전체 힙이 되어서는 안 된다는 것을 의미하고, 그렇지 않으면 TeraCache를 사용하면 오버헤드가 커진다.  

예를 들어 TeraCache의 접근 방식은 새로운 포인터를 위해 수정된 후 전이적 폐쇄를 재스캔하는 등 개념적으로 가변 가능한 객체를 처리할 수 있지만, Spark에서 캐시된 객체는 불변이다.
이는 TeraCache 힙을 캐시로 관리하는 것을 더 쉽게 한다.  

최근, DRAM을 두가지 방향으로 확장하기 위한 상당한 양의 연구 활동이 있다: (1) NVMe(혹은 NVM) 장치에서 DRAM으로의 transparently caching 그리고 (2) 바이트 주소 지정이 가능한 NVM 혹은 블록 NVME 장치를 통해 시스템 물리주소를 확장(캐싱은 아닌).
두 가지 경우 모두, 큰 힙을 채용하는것이 큰 GC 오버헤드를 발생시킨다.
우리의 접근은 GC 오버헤드를 줄임으로써 수명이 짧고 비슷한 특성을 가진 객체들의 그룹을 관리하는데 상당한 성능적 이점을 보여준다. 
따라서 우리는 TeraCache가 이 두 디자인의 대안으로 사용될 수 있으리라 생각한다.  

## 5. Related Work


## 6. Conclusions
Spark 어플리케이션들은 종종, 특히 반복적인 연산을 진행할 때 중간 데이터를 cache한다.
하지만, 반복되는 Spark의 RDD들의 직렬화/역직렬화는 현재는 GC 오버헤드의 증가 없이는 줄일 수 없는 상당한 CPU 오버헤드를 유발한다.
우리는 이런 오버헤드가 고속 스토리지를 JVM 힙으로 확장함으로 해결될 수 있다고 생각한다.
우리는 JVM과 Spark를 기반으로 하고 고속 스토리지에 memory mapped 된 on-heap RDD 캐시를 사용하는 TeraCache를 제안한다.
TeraCache는 cached 된 RDD에 직접 접근을 제공하여,  cached된 객체에를 위한 GC 오버헤드와 serdes를 제거한다.
우리의 실험 결과는 TeraCache를 사용할 때 serdes를 사용하는것에 비해 ML 워크로드 성능이 약 37% 향상됨을 보여준다.
우리는 TeraCache가 아주 많은 변하지 않는 객체를 cache하는 다른 프레임워크들의 성능도 향상시킬 수 있을 것이라 기대한다.
